{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, professor Shi. strongly recommends that we should read the following reference books:\n",
    "  *非线性最优化基础 Fukushimu 科学出版社*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Descent Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is a iterative optimization algorithm for finding the minimum of a function. It's very effective to solving the convex optimization problems. The defination of convex function $f(\\boldsymbol{w})$ is as follows:\n",
    "$$\n",
    "f(t\\boldsymbol{w}_1 + (1-t)\\boldsymbol{w}_2) \\leq tf(\\boldsymbol{w}_1) + (1-t)f(\\boldsymbol{w}_2), \\quad \\forall \\boldsymbol{w}_1, \\boldsymbol{w}_2 \\in \\mathbb{R}^n,\\forall t\\in [0, 1] \\tag{1.1}\n",
    "$$\n",
    "In order to implement the gradient descent method, we follow the following steps:\n",
    "- Step 1. Initialize the parameters $\\boldsymbol{w}_0\\in W$ and set the learning rate $\\eta_{t}$.\n",
    "- Step 2. Gradient Descent: $\\boldsymbol{w}_{t+1}' = \\boldsymbol{w}_t - \\eta_{t}\\nabla f(\\boldsymbol{w}_t)$\n",
    "- Step 3. Projection $\\boldsymbol{w}_{t+1}=P_{w}\\boldsymbol{w}_{t+1}'$\n",
    "- Repeat step 2. and step 3. for T steps\n",
    "- Step 4. Return $\\bar{\\boldsymbol{w}}_{T}=\\frac{1}{T} \\sum_{t=1}^{T}\\boldsymbol{w}_{t}$<br>\n",
    "  \n",
    "Step 3. is to guarantee that the parameters $\\boldsymbol{w}$ is in the feasible set $W$. $P_{W}(\\boldsymbol{z})= \\argmin_{\\boldsymbol{x}\\in W}||\\boldsymbol{x}-\\boldsymbol{z}||$.\n",
    "If we take a constant learning rate $\\eta_{t}=\\eta$, and the function $f(\\boldsymbol{w})$ is l-Lipschitz continuous and is bounded, then we have the following theorem:\n",
    ">**Theorem 1.1**. $f(\\boldsymbol{w})-\\min_{\\boldsymbol{w}\\in W}f(\\boldsymbol{w})\\leq O(\\frac{1}{\\sqrt{T}})$, where $\\eta=\\Gamma/(l/ \\sqrt{T})$ and $\\Gamma$ is the Diameter of $W$ and l is the upper bound of the length of gradient $||\\triangledown f||\\leq l$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conjugate Gradient Method\n",
    "Sometimes linear equations $Ax=b$ can be transform into the form of solving a minimum problem $x=\\min_{x}f,f=\\frac{1}{2}x^{T}Ax-b^{T}x$ <br>\n",
    "It's easy to know that the gradient of $f$ is $\\triangledown f=Ax-b$ and the Hessian matrix is $A$.<br> Here we suppose that $A$ is a symmetric positive definite matrix. Then we can use the conjugate gradient method to solve the linear equations $Ax=b$. <br>\n",
    "Before we show the process, we first introduce the following definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**definition2.1** The vectors $p_{0}, p_{1}, \\cdots, p_{k}$ are called conjugate with respect to the symmetric positive definite matrix $A$, or A-conjugate. if $p_{i}^{T}Ap_{j}=<p_{i},Ap_{j}>\\equiv<p_{i},p_{j}>_{A}=0$ for $i\\neq j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement the conjugate gradient method, we follow the following steps:\n",
    "- Step 1. Initialize the $x_{0}$ and the residual $r_{0}=b-Ax_{0}$, $p_{0}=r_{0}$, $k=0$.\n",
    "- Step 2. Compute the step size $\\alpha_{k}=\\frac{< r_{k},p_{k}>}{<p_{k},p_{k}>_{A}}$. Update the variables as follows:\n",
    "  $$ x_{k+1}=x_{k}+\\alpha_{k}p_{k} $$   \n",
    "  $$ r_{k+1}=r_{k}-\\alpha_{k}Ap_{k}$$  \n",
    "  $$ \\textcolor{blue}{p_{k+1}=r_{k+1}-\\sum_{l=0}^{k}\\frac{<r_{k+1},p_{l}>_{A}}{<p_{l},p_{l}>_{A}}p_{l}\\equiv r_{k+1}-\\sum_{l=0}^{k}\\beta_{l}p_{l}}$$ \n",
    "  or\n",
    "  $$ \\textcolor{red}{\\beta_{k}=\\frac{<r_{k+1},p_{k}>_{A}}{<p_{k},p_{k}>_{A}}} $$\n",
    "  $$ \\textcolor{red}{p_{k+1}=r_{k+1}-\\beta_{k}p_{k}} $$\n",
    "  $$ k \\rightarrow k+1 $$\n",
    "- Step 3. Repeat step 2. until $||r_{k}||<\\epsilon$ .\n",
    "The details can be found in the `CGM.solve()` function in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some explainations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can find a set of A-conjugate vectors $p_{1},\\dots,p_{n}$, then we can express the solution $x$ as a linear combination of $p_{1},\\dots,p_{n}$, i.e., $$x=\\sum_{i=1}^{n}\\alpha_{i}p_{i}$$.<br>\n",
    "Then we can get the following equation by substituting the equation into $Ax=b$:\n",
    "$$\n",
    "\\sum_{i=1}^{n}\\alpha_{i}Ap_{i}=b\n",
    "$$\n",
    "And we can get the coefficient $\\alpha_{j}$ by ordinary inner product with $p_{j}$ on both sides of the equation:\n",
    "$$\n",
    "\\alpha_{j}=\\frac{<b,p_{j}>}{<p_{j},p_{j}>_{A}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is, how do we find the A-conjugate vectors $p_{1},\\dots,p_{n}$? <br>\n",
    "We can find the answer by extending the **Gram-Schmidt method**. Suppose we have found $p_{1},\\dots,p_{k}$, and  $y_{k}$ which is not a linear combination of these vectors(guaranting $p_{k+1}\\neq 0$), then we can find $p_{k+1}$ by the following equation:\n",
    "$$\n",
    "\\textcolor{blue}{p_{k+1}=y_{k}-\\sum_{i=1}^{k}\\frac{<y_{k},p_{i}>_{A}}{<p_{i},p_{i}>_{A}}p_{i}}\n",
    "$$\n",
    "Take $y_{k}=r_{k}$ we have the standard conjugate gradient method to update $p_{k}$. <br>\n",
    "It's still not optimal method, since it requires to memorize all the vectors $p_{1},\\dots,p_{k}$, which is not efficient. <br>\n",
    "We can find a better method by the following equation:\n",
    "$$\n",
    "\\textcolor{red}{p_{k+1}={y_{k}-\\beta_{k}p_{k}}}\n",
    "$$\n",
    "To make sure $p_{k+1}\\perp^{A}p_{k}$, we perform A-inner product to both sides of the equation to determine $\\beta_{k}$:\n",
    "$$\n",
    "\\textcolor{red}{\\beta_{k}=\\frac{<y_{k},p_{k}>_{A}}{<p_{k},p_{k}>_{A}}}\n",
    "$$\n",
    "It's worth noting that the equation is different from the equation in the book, and the generated direction $p_{k+1}$ **is not** necessarily A-conjugate with $p_{j},j<k$, and this kind of conjugate gradient method can takes more steps then the standard conjugate gradient method to reach a higher accuracy. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Theorem 2.1**. $r_{l}\\perp span\\{r_{0}, \\dots,r_{l-1}\\}$,$p_{l}\\perp^{A}span\\{p_{0},p_{1},\\dots,p_{l-1}\\}$ and $span\\{p_{0},\\dots,p_{l-1}\\}=span\\{ r_{0},\\dots,r_{l-1}\\}=span\\{ r_{0},Ar_{0},A^{l-1}r_{0}\\}$\n",
    "\n",
    "Proof: Mathematical Induction:<br>\n",
    "1. base case: $l=1$  Obviously $span\\{ p_{0}\\}=span\\{r_{0}\\}$ because $p_{0}=r_{0}$<br>\n",
    "   $r_{1}\\perp span\\{r_{0}\\} \\Leftrightarrow <r_{0}-\\alpha_{0} Ap_{0},r_{0}>=0 $\n",
    "   $  \\Leftrightarrow \\alpha_{0}=\\frac{<r_{0},r_{0}>}{<p_{0},r_{0}>_{A}}$ \n",
    "   By Definition $ \\alpha_{0} =\\frac{<b,p_{0}>}{<p_{0},p_{0}>_{A}}$<br>\n",
    "   $p_{1}\\perp^{A}span\\{p_{0}\\} \\Leftrightarrow <r_{1}-\\beta_{0}p_{0},p_{0}>_{A}=0\\Leftrightarrow \\beta_{0}=\\frac{<r_{1},p_{0}>_{A}}{<p_{0},p_{0}>_{A}}$ \n",
    "   It holds by the definition of $\\beta_{0}$ <br>\n",
    "2. Induction hypothesis: assume when $l=k$ the theorem holds<br> when it comes to $l=k+1$<br>\n",
    "   First, $r_{k+1}\\perp span\\{r_{0},\\dots,r_{k}\\} \\Leftrightarrow <r_{k}-\\alpha_{k} Ap_{k},r_{j}>=0 , j=0,\\dots,k $<br>\n",
    "   for $ j<k $, from the induction hypothesis $ <r_{k},r_{j}>=0$, $<p_{k},r_{j}>_{A}=0$ $\\therefore <r_{k}-\\alpha_{k} Ap_{k},r_{j}>=<r_{k},r_{j}>-\\alpha_{k} <p_{k},r_{j}>_{A}=0$\n",
    "   for $j=k$,$<r_{k}-\\alpha_{k} Ap_{k},r_{j}>=0 \\Leftrightarrow \\alpha_{k}=\\frac{<r_{k},r_{k}>}{<p_{k},r_{k}>_{A}}$<br>\n",
    "   $\\because <r_{k},r_{k}>=<r_{k},p_{k}+\\sum_{l=0}^{k-1}\\beta_{l}p_{l}>=<r_{k},p_{k}>$ and $<p_{k},r_{k}>_{A}=<p_{k},r_{k}-\\sum_{l=0}^{k-1}\\beta_{l}p_{l}>_{A}=<p_{k},p_{k}>_{A}$<br>\n",
    "   $\\therefore \\alpha_{k}=\\frac{< r_{k},p_{k}>}{<p_{k},p_{k}>_{A}}=\\frac{<r_{k},r_{k}>}{<p_{k},r_{k}>_{A}}$<br>\n",
    "\n",
    "   Second, $p_{k+1}\\perp^{A}span\\{p_{0},p_{1},\\dots,p_{k}\\}$ is obvious by performing A-inner product with $p_{j},j=0,\\dots,k$<br>\n",
    "   \n",
    "   In order to prove that $span\\{p_{0},\\dots,p_{k}\\}=span\\{ r_{0},\\dots,r_{k}\\}$, we only need to prove that $p_{k}$ can be expressed by linear combination of $r_{0},\\dots,r_{k}$, because it's proved that vectors that are perpendicular or A-conjugate to each other are linearly independent.<br>\n",
    "   $\\because p_{k}=r_{k}-\\sum_{l=0}^{k-1}\\beta_{l} p_{l}$ and from induction hypothesis $p_{l}$ can be expressed by linear combination of $r_{0},\\dots,r_{l},l=0,\\dots,k-1$<br>\n",
    "   $\\therefore p_{k}$ can be expressed by linear combination of $r_{0},\\dots,r_{k}$<br>\n",
    "   Finally, to prove that $span\\{ r_{0},Ar_{0},A^{k}r_{0}\\}= span\\{r_{0}, \\dots,r_{k}\\}$ We denote that if a vector $x$ can be expressed by linear combination of $x_{0}, \\dots,x_{k}$, then $x=L(x_{0}, \\dots,x_{k})$<br>\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   &r_{l+1}=r_{l}-\\alpha_{l}Ap_{l}=r_{l}-\\alpha_{l}A(r_{l}-\\sum_{i=0}^{l-1}\\beta_{i}p_{i})\\\\\n",
    "   &\\Rightarrow Ar_{l}=\\frac{1}{\\alpha_{l}}(r_{l}-r_{l+1})+AL(p_{0},\\dots,p_{l-1})\\\\\n",
    "   &=L(r_{l},\\textcolor{red}{r_{l+1}})+L(Ap_{0},\\dots,Ap_{l-1})\\\\\n",
    "   &=L(r_{l},\\textcolor{red}{r_{l+1}})+L(r_{1}-r_{0},\\dots,r_{l}-r_{l-1})\\\\\n",
    "   &=L(r_{0},\\dots,r_{l},\\textcolor{red}{r_{l+1}})\\\\\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "   We didn't use the induction hypothesis here, so it holds for all $l=0,\\dots,k$. It's worth mentioning that the coefficient of $r_{l+1}\\neq 0$ in the expression $Ar_{l}=L(r_{0},\\dots,r_{l+1})$, so I use red color to mark it.<br>\n",
    "   Now,\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   &A^{l}r_{0}=A^{l-1}L(r_{0},\\textcolor{red}{r_{1}})\\\\\n",
    "   &          =A^{l-2}L(Ar_{0},\\textcolor{red}{Ar_{1}})\\\\\n",
    "   &          =A^{l-2}L(L(r_{0},\\textcolor{red}{r_{1}}),L(r_{0},r_{1},\\textcolor{red}{r_{2}}))\\\\\n",
    "   &          =A^{l-2}L(r_{0},r_{1},\\textcolor{red}{r_{2}})\\\\\n",
    "   &          =\\dots\\\\\n",
    "   &          =L(r_{0},\\dots,\\textcolor{red}{r_{l}}),l=0,\\dots,k\\\\\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "   So we can conclude that $A^{k}r_{0}$ can be linearly expressed by $ r_{0},r_{1},\\dots,r_{k}$ Still, the coefficient of $r_{k}\\neq 0$ in the expression.<br>\n",
    "   And $A^{k}r_{0}$ is linearly independent with $r_{0},Ar_{0},\\dots,A^{k-1}r_{0}$. Otherwise, we have:\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   &A^{k}r_{0}=L(r_{0},\\dots,A^{k-1}r_{0})\\\\\n",
    "   &=L(r_{0},\\dots,r_{k-1})\\\\\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "   But $ A^{k}r_{0}=L(r_{0},\\dots,\\textcolor{red}{r_{k}})$ with nonzero coefficient of $r_{k}$, meaning that $r_{k}=L(r_{0},\\dots,r_{k-1})$ which is a contradiction.<br>\n",
    "   Therefore, we can inductively prove that $r_{0},Ar_{0},\\dots,A^{k}r_{0}$ are linear independent so that $span\\{ r_{0},Ar_{0},\\dots,A^{k}r_{0}\\}= span\\{r_{0}, \\dots,r_{k}\\}$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGM():\n",
    "    def __init__(self,A,b):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.x = np.zeros_like(b)\n",
    "    def solve(self,A=None,b=None,Gram_Schmidt=False,record_list=False):\n",
    "        if A is not None:\n",
    "            self.A = A\n",
    "        if b is not None:\n",
    "            self.b = b\n",
    "            self.x = np.zeros_like(b)\n",
    "        r = self.b - np.dot(self.A,self.x)\n",
    "        p = r\n",
    "        steps = 0\n",
    "        if Gram_Schmidt==True:\n",
    "            p_list = []\n",
    "            Ap_list = []\n",
    "            p_list.append(p)\n",
    "            while np.linalg.norm(r) > 1e-10 and steps<self.A.shape[0]:\n",
    "                Ap = np.dot(self.A,p)\n",
    "                Ap_list.append(Ap)\n",
    "                alpha = np.dot(self.b,p)/np.dot(p,Ap)\n",
    "                #alpha = np.dot(r,r)/np.dot(p,Ap) #way3 to calculate alpha\n",
    "                self.x = self.x + alpha*p\n",
    "                r_temp = r\n",
    "                r = r - alpha*Ap\n",
    "                p = r\n",
    "                for i in range(len(p_list)):\n",
    "                    p = p - np.dot(r,Ap_list[i])/np.dot(p_list[i],Ap_list[i])*p_list[i]\n",
    "                #print(np.dot(p,Ap))\n",
    "                p_list.append(p)\n",
    "                steps+=1\n",
    "            if record_list==True:\n",
    "                self.p_list = p_list\n",
    "                self.Ap_list = Ap_list\n",
    "        else:\n",
    "            if record_list==True:\n",
    "                p_list = []\n",
    "                Ap_list = []\n",
    "                p_list.append(p)\n",
    "            while np.linalg.norm(r) > 1e-10:\n",
    "                Ap = np.dot(self.A,p)\n",
    "                if record_list==True:\n",
    "                    Ap_list.append(Ap)\n",
    "                #alpha = np.dot(r,p)/np.dot(p,Ap)\n",
    "                #alpha = np.dot(self.b,p)/np.dot(p,Ap) #way2 to calculate alpha\n",
    "                alpha = np.dot(r,r)/np.dot(p,Ap) #way3 to calculate alpha\n",
    "                self.x = self.x + alpha*p\n",
    "                r_temp = r\n",
    "                r = r - alpha*Ap   \n",
    "                beta = np.dot(r,r)/np.dot(r_temp,r_temp)\n",
    "                #beta = -np.dot(r,Ap)/np.dot(p,Ap)\n",
    "                p = r + beta*p\n",
    "                if record_list==True:\n",
    "                    p_list.append(p)\n",
    "                steps+=1\n",
    "            if record_list==True:\n",
    "                self.p_list = p_list\n",
    "                self.Ap_list = Ap_list\n",
    "        self.steps = steps\n",
    "\n",
    "        return self.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[2,1,0],\n",
    "              [1,2,0.5],\n",
    "              [0,0.5,1]])#A is symmetric and positive definite\n",
    "b = np.array([3,4,1.0])\n",
    "cgm=CGM(A,b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7, 1.6, 0.2])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cgm.solve(Gram_Schmidt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 4., 1.])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(A,cgm.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A100 = np.random.rand(100,100)\n",
    "#A100 = np.dot(A100.T,A100)\n",
    "#b100 = np.random.rand(100)\n",
    "A100 = np.load('./data/A100.npy')\n",
    "b100 = np.load('./data/b100.npy')\n",
    "x100_np_linalg_solve=np.linalg.solve(A100,b100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution error by numpy: 1.3394030069294292e-10\n",
      "solution error by CGM without Gram_Schmidt: 1.245655570793503e-10\n",
      "solution steps by CGM without Gram_Schmidt: 209\n"
     ]
    }
   ],
   "source": [
    "x100_cgm_solve=cgm.solve(A100,b100,Gram_Schmidt=False,record_list=True)\n",
    "print(f'solution error by numpy: {np.linalg.norm(np.dot(A100,x100_np_linalg_solve)-b100)}')\n",
    "print(f'solution error by CGM without Gram_Schmidt: {np.linalg.norm(np.dot(A100,x100_cgm_solve)-b100)}')\n",
    "print(f'solution steps by CGM without Gram_Schmidt: {cgm.steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution error by numpy: 1.3394030069294292e-10\n",
      "solution error by CGM with Gram_Schmidt: 6.484412297666219e-10\n",
      "solution steps by CGM with Gram_Schmidt: 100\n"
     ]
    }
   ],
   "source": [
    "x100_cgm_solve=cgm.solve(A100,b100,Gram_Schmidt=True,record_list=True)\n",
    "print(f'solution error by numpy: {np.linalg.norm(np.dot(A100,x100_np_linalg_solve)-b100)}')\n",
    "print(f'solution error by CGM with Gram_Schmidt: {np.linalg.norm(np.dot(A100,x100_cgm_solve)-b100)}')\n",
    "print(f'solution steps by CGM with Gram_Schmidt: {cgm.steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.805852385788199e-13\n",
      "97.15625118834603\n",
      "6.718641725959403e-13\n",
      "-2.40455594709508e-10\n",
      "8.052655063280934e-08\n",
      "-2.9214271034967522e-05\n",
      "0.008658304451625676\n",
      "-3.0363528805898143\n",
      "371.7791040055364\n",
      "-0.014809322889929663\n"
     ]
    }
   ],
   "source": [
    "x100_cgm_solve=cgm.solve(A100,b100,Gram_Schmidt=False,record_list=True)\n",
    "for i in range(10):\n",
    "    print(np.dot(cgm.p_list[1],cgm.Ap_list[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, we can know that the conjugate gradient method without schmidt process can also converge to the solution of the linear equations $Ax=b$ with a higher accuracy. But the directions $p_{i}$ are not necessarily A-conjugate with each other. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
