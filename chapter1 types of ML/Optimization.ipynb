{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, professor Shi. strongly recommends that we should read the following reference books:\n",
    "  *非线性最优化基础 Fukushimu 科学出版社*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Descent Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is a iterative optimization algorithm for finding the minimum of a function. It's very effective to solving the convex optimization problems. The defination of convex function $f(\\bm{w})$ is as follows:\n",
    "$$\n",
    "f(t\\bm{w}_1 + (1-t)\\bm{w}_2) \\leq tf(\\bm{w}_1) + (1-t)f(\\bm{w}_2), \\quad \\forall \\bm{w}_1, \\bm{w}_2 \\in \\mathbb{R}^n,\\forall t\\in [0, 1] \\tag{1.1}\n",
    "$$\n",
    "In order to implement the gradient descent method, we follow the following steps:\n",
    "- Step 1. Initialize the parameters $\\bm{w}_0\\in W$ and set the learning rate $\\eta_{t}$.\n",
    "- Step 2. Gradient Descent: $\\bm{w}_{t+1}' = \\bm{w}_t - \\eta_{t}\\nabla f(\\bm{w}_t)$\n",
    "- Step 3. Projection $\\bm{w}_{t+1}=P_{w}\\bm{w}_{t+1}'$\n",
    "- Repeat step 2. and step 3. for T steps\n",
    "- Step 4. Return $\\bar{\\bm{w}}_{T}=\\frac{1}{T} \\sum_{t=1}^{T}\\bm{w}_{t}$<br>\n",
    "  \n",
    "Step 3. is to guarantee that the parameters $\\bm{w}$ is in the feasible set $W$. $P_{W}(\\bm{z})= \\argmin_{\\bm{x}\\in W}||\\bm{x}-\\bm{z}||$.\n",
    "If we take a constant learning rate $\\eta_{t}=\\eta$, and the function $f(\\bm{w})$ is l-Lipschitz continuous and is bounded, then we have the following theorem:\n",
    ">**Theorem 1.1**. $f(\\bm{w})-\\min_{\\bm{w}\\in W}f(\\bm{w})\\leq O(\\frac{1}{\\sqrt{T}})$, where $\\eta=\\Gamma/(l/ \\sqrt{T})$ and $\\Gamma$ is the Diameter of $W$ and l is the upper bound of the length of gradient $||\\triangledown f||\\leq l$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conjugate Gradient Method\n",
    "Sometimes linear equations $Ax=b$ can be transform into the form of solving a minimum problem $x=\\min_{x}f,f=\\frac{1}{2}x^{T}Ax-b^{T}x$ <br>\n",
    "It's easy to know that the gradient of $f$ is $\\triangledown f=Ax-b$ and the Hessian matrix is $A$.<br> Here we suppose that $A$ is a symmetric positive definite matrix. Then we can use the conjugate gradient method to solve the linear equations $Ax=b$. <br>\n",
    "Before we show the process, we first introduce the following definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**definition2.1** The vectors $p_{0}, p_{1}, \\cdots, p_{k}$ are called conjugate with respect to the symmetric positive definite matrix $A$, or A-conjugate. if $p_{i}^{T}Ap_{j}=<p_{i},Ap_{j}>\\equiv<p_{i},p_{j}>_{A}=0$ for $i\\neq j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement the conjugate gradient method, we follow the following steps:\n",
    "- Step 1. Initialize the $x_{0}$ and the residual $r_{0}=b-Ax_{0}$, $p_{0}=r_{0}$, $k=0$.\n",
    "- Step 2. Compute the step size $\\alpha_{k}=\\frac{< r_{k},p_{k}>}{<p_{k},p_{k}>_{A}}$. Update the variables as follows:\n",
    "  $$ x_{k+1}=x_{k}+\\alpha_{k}p_{k} $$   \n",
    "  $$ r_{k+1}=r_{k}-\\alpha_{k}Ap_{k}$$  \n",
    "  $$ \\beta_{k}=\\frac{<r_{k+1},p_{k}>_{A}}{<p_{k},p_{k}>_{A}} $$\n",
    "  $$ p_{k+1}=r_{k+1}-\\beta_{k}p_{k} $$\n",
    "  $$ k \\rightarrow k+1 $$\n",
    "- Step 3. Repeat step 2. until $||r_{k}||<\\epsilon$ .\n",
    "The details can be found in the `CGM.solve()` function in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some explainations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can find a set of A-conjugate vectors $p_{1},\\dots,p_{n}$, then we can express the solution $x$ as a linear combination of $p_{1},\\dots,p_{n}$, i.e., $$x=\\sum_{i=1}^{n}\\alpha_{i}p_{i}$$.<br>\n",
    "Then we can get the following equation by substituting the equation into $Ax=b$:\n",
    "$$\n",
    "\\sum_{i=1}^{n}\\alpha_{i}Ap_{i}=b\n",
    "$$\n",
    "And we can get the coefficient $\\alpha_{j}$ by ordinary inner product with $p_{j}$ on both sides of the equation:\n",
    "$$\n",
    "\\alpha_{j}=\\frac{<b,p_{j}>}{<p_{j},p_{j}>_{A}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is, how do we find the A-conjugate vectors $p_{1},\\dots,p_{n}$? <br>\n",
    "We can find the answer by extending the **Gram-Schmidt method**. Suppose we have found $p_{1},\\dots,p_{k}$, and  $y_{k}$ which is not a linear combination of these vectors(guaranting $p_{k+1}\\neq 0$), then we can find $p_{k+1}$ by the following equation:\n",
    "$$\n",
    "p_{k+1}=y_{k}-\\sum_{i=1}^{k}\\frac{<y_{k},p_{i}>_{A}}{<p_{i},p_{i}>_{A}}p_{i}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Theorem 2.1**. $r_{l}\\perp span\\{r_{0}, \\dots,r_{l-1}\\}$,$p_{l}\\perp^{A}\\{p_{0},p_{1},\\dots,p_{l-1}\\}$ and $span\\{p_{0},\\dots,p_{l-1}\\}=span\\{ r_{0},\\dots,r_{l-1}\\}=span\\{ r_{0},Ar_{0},A^{l-1}r_{0}\\}$\n",
    "\n",
    "proof: Mathematical Induction:<br>\n",
    "1. base case: $l=1$  Obviously $span\\{ p_{0}\\}=span\\{r_{0}\\}$ because $p_{0}=r_{0}$<br>\n",
    "   $r_{1}\\perp span\\{r_{0}\\} \\Leftrightarrow <b-Ax_{1},>$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGM():\n",
    "    def __init__(self,A,b):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.x = np.zeros_like(b)\n",
    "    def solve(self,A=None,b=None):\n",
    "        if A is not None:\n",
    "            self.A = A\n",
    "        if b is not None:\n",
    "            self.b = b\n",
    "        r = self.b - np.dot(self.A,self.x)\n",
    "        p = r\n",
    "        steps = 0\n",
    "        while np.linalg.norm(r) > 1e-10:\n",
    "            Ap = np.dot(self.A,p)\n",
    "            alpha = np.dot(r,p)/np.dot(p,Ap)\n",
    "            #alpha = np.dot(self.b,p)/np.dot(p,Ap) way2 to calculate alpha\n",
    "            #alpha = np.dot(r,r)/np.dot(p,Ap) way3 to calculate alpha\n",
    "            print(f'dot(r,p) = {np.dot(r,p)}')\n",
    "            print(f'dot(r,r) = {np.dot(r,r)}')\n",
    "            print(f'dot(b,p) = {np.dot(b,p)}')\n",
    "            self.x = self.x + alpha*p\n",
    "            r = r - alpha*Ap\n",
    "            beta = np.dot(r,r)/np.dot(self.b,self.b)\n",
    "            p = r + beta*p\n",
    "            steps+=1\n",
    "        self.steps = steps\n",
    "        return self.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[2,1],\n",
    "              [1,2]])#A is symmetric and positive definite\n",
    "b = np.array([3,4])\n",
    "cgm=CGM(A,b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66666667, 1.66666667])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cgm.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3., 1.]),\n",
       " array([[ 0.70710678, -0.70710678],\n",
       "        [ 0.70710678,  0.70710678]]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CGM' object has no attribute 'steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cgm\u001b[39m.\u001b[39;49msteps\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CGM' object has no attribute 'steps'"
     ]
    }
   ],
   "source": [
    "cgm.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgm.A=np.array([[1,1],[2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
