{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiple linear regression tasks, the action function is mutiple regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preparation\n",
    "Here we prepare a dataset $\\{ (\\boldsymbol{x}_{i},y_{i})\\},i=1,\\dots,m$, where $\\boldsymbol{x}_{i}=(x_{i1},\\dots,x_{id})^{\\top}\\in\\mathbb{R}^{d}$ and $y_{i}\\in\\mathbb{R}$.<br>\n",
    "In the training set, we prepare $m=100$ samples with feature dimension $d=2$. Then we create a matrix $X_{train}=(x_{1}^{T},\\dots,x_{m}^{T})^{T}\\in\\mathbb{R}^{m\\times d}$<br>\n",
    "We set the real parameters $\\boldsymbol{w}_{real}=(1,2)^{T} ,b=4,\\boldsymbol{b}=(b,\\dots,b)^{T}$ for testing.<br>\n",
    "And $\\boldsymbol{Y} = \\boldsymbol{X}_{train}\\boldsymbol{w}_{real}+\\boldsymbol{b}+Noise$ is the labels in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "def create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        # 如果文件夹不存在，则创建它\n",
    "        os.makedirs(folder_path)\n",
    "        print(\"文件夹已创建\")\n",
    "    else:\n",
    "        print(\"文件夹已存在，不需要创建\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件夹已创建\n"
     ]
    }
   ],
   "source": [
    "create_folder(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 2), (100, 1), (2, 1), (1, 1))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dim = 2\n",
    "train_m = 100\n",
    "X_train = np.random.rand(train_m,feature_dim)#m*d\n",
    "w_real = np.array([[1], [2]])\n",
    "b = np.array([[4]])\n",
    "Y = X_train@w_real+ b + np.random.randn(train_m,1)*0.01\n",
    "X_train.shape, Y.shape, w_real.shape, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the rule of vector derivative\n",
    "Before deriving the optimal paramter $w$, we firstly definne the rules for vector derivative<br>\n",
    "if we have a scalar $F=F(\\boldsymbol{x})$ where the independent varible $\\boldsymbol{x}\\in\\mathbb{R}^{d}$, then the derivative formula of $F$ with respect to $\\boldsymbol{x}$ is as follows:<br>\n",
    "$$ \\frac{\\partial F}{\\partial \\boldsymbol{x}}= [\\frac{\\partial F}{\\partial x_{1}},\\dots,\\frac{\\partial F}{\\partial x_{d}}]^{T}\\in\\mathbb{R}^{d} $$\n",
    "if we have a vector $\\boldsymbol{F}=\\boldsymbol{F}(\\boldsymbol{x})=[F_{1}(\\boldsymbol{x}),\\dots,F_{1}(\\boldsymbol{x})]^{T}\\in\\mathbb{R}^{m}$ where the independent varible $\\boldsymbol{x}\\in\\mathbb{R}^{d}$, then the derivative formula of $\\boldsymbol{F}$ with respect to $\\boldsymbol{x}$ is as follows:<br>\n",
    "$$ \\frac{\\partial \\boldsymbol{F}}{\\partial \\boldsymbol{x}}= [\\frac{\\partial F_{1}}{\\partial \\boldsymbol{x}},\\dots,\\frac{\\partial F_{m}}{\\partial \\boldsymbol{x}}]\\in\\mathbb{R}^{d\\times m}$$\n",
    "Under such defination, we have\n",
    "$$\n",
    "\\frac{\\partial (\\boldsymbol{A}\\boldsymbol{x})}{\\partial \\boldsymbol{x}}=\\boldsymbol{A}^{T}, \\frac{\\partial (\\boldsymbol{x}^{T}\\boldsymbol{A})}{\\partial \\boldsymbol{x}}=\\boldsymbol{A}, \\frac{\\partial (\\boldsymbol{x}^{T}\\boldsymbol{x})}{\\partial \\boldsymbol{x}}=2\\boldsymbol{x}, \\frac{\\partial (\\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x})}{\\partial \\boldsymbol{x}}=(\\boldsymbol{A}+\\boldsymbol{A}^{T})\\boldsymbol{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### derivation of optimal parameter $w$\n",
    "Now we can derive the optimal parameter $w$ for multiple linear regression.<br>\n",
    "We use the least square loss principle: the residual sum of squares (RSS) is minimized.<br>\n",
    "$$ RSS(\\bar{\\boldsymbol{w}})=||\\hat{\\boldsymbol{y}}-\\boldsymbol{y}||^{2}_{2}=||\\boldsymbol{y}-\\bar{\\boldsymbol{X}}\\bar{\\boldsymbol{w}}||^{2}_{2}$$\n",
    "$$ \\frac{\\partial RSS(\\bar{\\boldsymbol{w}})}{\\partial \\bar{\\boldsymbol{w}}}= \\frac{\\partial \\{ (\\boldsymbol{y}-\\bar{\\boldsymbol{X}}\\bar{\\boldsymbol{w}})^{T}(\\boldsymbol{y}-\\bar{\\boldsymbol{X}}\\bar{\\boldsymbol{w}})\\} }{\\partial \\bar{\\boldsymbol{w}}} = 2\\bar{\\boldsymbol{X}}^{T}(\\bar{\\boldsymbol{X}}\\bar{\\boldsymbol{w}}-\\boldsymbol{y}) $$\n",
    "Here, $\\bar{\\boldsymbol{w}}=(\\boldsymbol{w},b)\\in\\mathbb{R}^{d+1}$ represents the expanded vector , and $\\bar{\\boldsymbol{X}}\\in\\mathbb{R}^{m \\times (d+1) }$ is the expanded matrix with $1$ as the **last column**.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial RSS(\\bar{\\boldsymbol{w}})}{\\partial \\bar{\\boldsymbol{w}}}= 0 \\Rightarrow \\bar{\\boldsymbol{w}}=(\\boldsymbol{X}^{T} \\boldsymbol{X})^{-1}\\boldsymbol{X}^{T}\\boldsymbol{y} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bar = np.hstack((X_train,np.ones((m,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99906013],\n",
       "       [2.00311505],\n",
       "       [3.99771095]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = (np.linalg.inv(X_bar.T@X_bar))@X_bar.T@Y\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this result we can see that the optimal parameter $\\bar{\\boldsymbol{w}}$ is very close to the real parameter $\\bar{\\boldsymbol{w}}_{real}=(w_{real},b)=((1,2),3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
