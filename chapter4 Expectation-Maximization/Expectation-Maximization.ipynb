{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation-Maximization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: - [Expectation-Maximization Algorithm - Wikipedia ](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention: The note I use in this notebook maybe different from the textbook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Given the statistical model which generates a set of observed data $\\bm{X}$, a set of unobserved latent data or missing values $\\bm{Z}$, and a vector of unknown parameters $\\bm{\\theta}$, along with a likelihood function $L(\\bm{\\theta}; \\bm{X}, \\bm{Z}) = p(\\bm{X}, \\bm{Z} | \\bm{\\theta})$, the maximum likelihood estimate (MLE) of the unknown parameters is determined by maximizing the marginal likelihood of the observed data\n",
    "$$\n",
    "L(\\bm{\\theta}; \\bm{X}) = p(\\bm{X} | \\bm{\\theta}) = \\int_{\\bm{Z}} p(\\bm{X}, \\bm{Z} | \\bm{\\theta}) d\\bm{Z} = \\int_{\\bm{Z}} p(\\bm{X} | \\bm{Z}, \\bm{\\theta}) p(\\bm{Z} | \\bm{\\theta}) d\\bm{Z}\n",
    "$$\n",
    "However, this quantity is often intractable since $\\bm{Z}$ is unoberseved and the distribution of $\\bm{Z}$ is unknown before attaining $\\bm{\\theta}$.<br>\n",
    "Under such situation, we define $(\\bm{X},\\bm{Z})$ as complete data and $L_{c}(\\bm{\\theta};\\bm{X},\\bm{Z})$ as complete-data log likelihood. For any distribution $q(\\bm{Z})$, we have\n",
    "$$\n",
    "\\left< L_{c}(\\bm{\\theta};\\bm{X},\\bm{Z})\\right>_{q} = \\int_{\\bm{Z}} q(\\bm{Z}) L_{c}(\\bm{\\theta};\\bm{X},\\bm{Z}) d\\bm{Z} = \\int_{\\bm{Z}} q(\\bm{Z}) \\log p(\\bm{X},\\bm{Z}|\\bm{\\theta}) d\\bm{Z}\n",
    "$$\n",
    "By Jensen's inequality, we have\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(\\bm{X}|\\bm{\\theta}) &= \\log \\int_{\\bm{Z}} p(\\bm{X},\\bm{Z}|\\bm{\\theta}) d\\bm{Z} \\\\\n",
    "&= \\log \\int_{\\bm{Z}} q(\\bm{Z}) \\frac{p(\\bm{X},\\bm{Z}|\\bm{\\theta})}{q(\\bm{Z})} d\\bm{Z} \\\\\n",
    "&\\geq \\int_{\\bm{Z}} q(\\bm{Z}) \\log \\frac{p(\\bm{X},\\bm{Z}|\\bm{\\theta})}{q(\\bm{Z})} d\\bm{Z} \\\\\n",
    "&= \\left<  L_{c}(\\bm{\\theta};\\bm{X},\\bm{Z})\\right>_{q} - \\left< \\log q(\\bm{Z}) \\right>_{q}\n",
    "\\end{align}\n",
    "$$\n",
    "Furthermore, we define $H(q) = - \\left< \\log q(\\bm{Z}) \\right>_{q}$, obviously $H(q)>0$, therefore we find a lower bound of $\\log p(\\bm{X}|\\bm{\\theta})$, it can be defined as free energy $F(q,\\bm{\\theta})$:\n",
    "$$\n",
    "F(q,\\bm{\\theta}) = \\left<  L_{c}(\\bm{\\theta};\\bm{X},\\bm{Z}) \\right>_{q} + H(q)\n",
    "$$\n",
    "The EM algorithm can be viewed as two alternating maximization steps, that is, as an example of **coordinate descent**. Consider the free energy function:\n",
    "$$\n",
    "F(q,\\bm{\\theta}) = E_{q}(L_{c}(\\bm{\\theta};\\bm{X},\\bm{Z}) ) + H(q)\n",
    "$$\n",
    "where $q$ is any distribution over the latent variables $\\bm{Z}$, and $H(q)$ is the entropy of $q$.<br>\n",
    "This function can be written as \n",
    "$$\n",
    "F(q,\\bm{\\theta}) = -KL(q(\\bm{Z})||p(\\bm{Z}|\\bm{X},\\bm{\\theta})) + \\log p(\\bm{X}|\\bm{\\theta})\n",
    "$$\n",
    "where $KL(q(\\bm{Z})||p(\\bm{Z}|\\bm{X},\\bm{\\theta}))$ is the Kullback-Leibler divergence between $q(\\bm{Z})$ and $p(\\bm{Z}|\\bm{X},\\bm{\\theta})$.<br>\n",
    "Then the steps in the EM algorithm may be viewed as:\n",
    "1. E-step: $q^{(t+1)}(\\bm{Z}) = \\arg \\max_{q(\\bm{Z})} F(q,\\bm{\\theta}^{(t)})$\n",
    "2. M-step: $\\bm{\\theta}^{(t+1)} = \\arg \\max_{\\bm{\\theta}} F(q^{(t+1)},\\bm{\\theta})$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we can prove that $q^{(t+1)} = p(\\bm{Z}|\\bm{X},\\bm{\\theta}^{(t)})$.\n",
    "Then we have $F(q^{(t+1)},\\bm{\\theta}) =  \\left<  L_{c}(\\bm{\\theta};\\bm{X},\\bm{Z}) \\right>_{q^{(t+1)}} + H(q^{(t+1)})$, and the M-step is to the first term of $F(q^{(t+1)},\\bm{\\theta})$. It can be presented as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bm{\\theta}^{(t+1)} &= \\arg \\max_{\\bm{\\theta}} F(q^{(t+1)},\\bm{\\theta}) \\\\\n",
    "&= \\arg \\max_{\\bm{\\theta}} \\left<  L_{c}(\\bm{\\theta};\\bm{X},\\bm{Z}) \\right>_{q^{(t+1)}} \\\\\n",
    "&= \\arg \\max_{\\bm{\\theta}} \\int_{\\bm{Z}} q^{(t+1)}(\\bm{Z}) \\log p(\\bm{X},\\bm{Z}|\\bm{\\theta}) d\\bm{Z} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "For simplicity, we define $Q(\\bm{\\theta},\\bm{\\theta}^{(t)}) = \\int_{\\bm{Z}} q^{(t+1)}(\\bm{Z}) \\log p(\\bm{X},\\bm{Z}|\\bm{\\theta}) d\\bm{Z}= \\int_{\\bm{Z}} p(\\bm{Z}|\\bm{X},\\bm{\\theta}^{(t)}) \\log p(\\bm{X},\\bm{Z}|\\bm{\\theta}) d\\bm{Z}$, then we have the equivlent form of EM algorithm:\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\text{E-Step:  } Q(\\bm{\\theta},\\bm{\\theta}^{(t)}) = \\int_{\\bm{Z}} p(\\bm{Z}|\\bm{X},\\bm{\\theta}^{(t)}) \\log p(\\bm{X},\\bm{Z}|\\bm{\\theta}) d\\bm{Z} = E_{\\bm{Z}|\\bm{X},\\bm{\\theta}^{(t)}}(log L(\\bm{\\theta};\\bm{X},\\bm{Z})) \\\\\n",
    "&\\text{M-Step:  } \\bm{\\theta}^{(t+1)} = \\arg \\max_{\\bm{\\theta}} Q(\\bm{\\theta},\\bm{\\theta}^{(t)})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
