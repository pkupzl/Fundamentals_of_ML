{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络求解特征值问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、问题描述\n",
    "在微分方程中，我们往往能够遇到一类问题，即求解特征值问题。典型的问题有Sturm-Liouville问题，其形式如下：\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac{d}{dx}\\left(p(x)\\frac{dy}{dx}\\right)+q(x)y+\\lambda w(x)y=0, \\quad x\\in[a,b]\\\\\n",
    "\\alpha_{1}y(a)+\\beta_{1}y'(a)=0\\\\\n",
    "\\alpha_{2}y(b)+\\beta_{2}y'(b)=0\n",
    "\\end{cases}\n",
    "$$\n",
    "其中，$p(x),q(x),w(x)$是已知函数，$\\alpha_1,\\beta_1,\\alpha_2,\\beta_2$是已知常数，$\\lambda$是待求特征值，$y(x)$是待求特征函数。可能的物理背景是量子力学。事实上取$p(x) = \\frac{h}{2m}$，$q(x) = V(x)$，$w(x) = 1$，$\\beta_{1}=\\beta_{2}  = 0$则上述方程变为：\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\left(-\\frac{\\hbar^2}{2m}\\frac{d^2}{dx^2}+V(x)\\right)y=Ey, \\quad x\\in[a,b]\\\\\n",
    "y(a)=y(b)=0\n",
    "\\end{cases}\n",
    "$$\n",
    "此即一维定态薛定谔方程，其中$E$是能量，$y(x)$是波函数，迪利克雷边界条件对应着粒子被限制在有限的区域内。\n",
    "另外，还有一类问题，即Laplace特征值问题，其形式如下：\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\Delta u+\\lambda u=0, \\quad x\\in\\Omega\\\\\n",
    "u|_{\\partial\\Omega}=0\n",
    "\\end{cases}\n",
    "$$\n",
    "其中，$\\Omega \\subset R^{n}$ 是一个有界的、具有光滑边界的开区域，$\\partial\\Omega$是边界，$\\Delta$是Laplace算子，$\\lambda$是待求特征值，$u(x)$是待求特征函数。\n",
    "事实上，对于更一般的椭圆型方程：\n",
    "$$\n",
    "\\begin{cases}\n",
    "-\\Delta u+U(x)u=\\lambda u, \\quad x\\in\\Omega\\\\\n",
    "u|_{\\partial\\Omega}=0\n",
    "\\end{cases}\n",
    "$$\n",
    "其中 $U(x)\\in C(\\bar{\\Omega})$通过泛函分析的方法，可以证明，方程的特征值都是实的，而且有可数个$\\lambda_1\\leq\\lambda_2\\leq\\cdots\\leq\\lambda_n\\leq\\cdots$，并且它们对应的特征函数构成空间 $H_{0}^{1}$ 的完备正交集。这里，$H_{0}^{1}$是满足边界条件的Sobolev空间，其定义为：\n",
    "$$\n",
    "H_{0}^{1}=\\left\\{u\\in H^{1}(\\Omega):\\quad u|_{\\partial\\Omega}=0\\right\\}\n",
    "$$\n",
    "其中，$H^{1}(\\Omega)$是Sobolev空间，其定义为：\n",
    "$$\n",
    "H^{1}(\\Omega)=\\left\\{u\\in L^{2}(\\Omega):\\quad \\frac{\\partial u}{\\partial x_{i}}\\in L^{2}(\\Omega),\\quad i=1,2,\\cdots,n\\right\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "证明：定义方程的弱解为$u\\in H_{0}^{1}$，满足：\n",
    "$$\n",
    "\\int_{\\Omega}\\left(\\nabla u\\cdot\\nabla v+U(x)uv\\right)dx=\\lambda\\int_{\\Omega}uvdx,\\quad \\forall v\\in H_{0}^{1}\n",
    "$$\n",
    "我们将上式化为对称紧算子问题，为此在$H_{0}^{1}$上引入等价范数：\n",
    "$$\n",
    "\\|u\\|_{e}=\\left(\\int_{\\Omega}\\left(|\\nabla u|^{2}+(U(x)+\\lambda_{0})u^{2}\\right)dx\\right)^{\\frac{1}{2}}\n",
    "$$\n",
    "其中，$\\lambda_{0}$是一个足够大的正数，使得$U(x)+\\lambda_{0}\\geq 0$。容易验证，$\\|\\cdot\\|_{e}$是$H_{0}^{1}$上的范数，且$H_{0}^{1}$在此范数下是完备的。再定义内积：\n",
    "$$\n",
    "(u,v)_{\\lambda_{0}}=\\int_{\\Omega}\\left(\\nabla u\\cdot\\nabla v+(U(x)+\\lambda_{0})uv\\right)dx\n",
    "$$\n",
    "易知$H_{0}^{1}(\\Omega)$在此内积下是Hilbert空间。记 $||\\cdot||$ 为$L^{2}(\\Omega)$上的范数，再由Poincare不等式可知，存在常数$C>0$，使得：\n",
    "$$\n",
    "\\|\\int_{\\Omega}u \\cdot v dx\\|\\leq C\\|u\\| \\cdot \\|v\\|_{e},\\quad \\forall v\\in H_{0}^{1}(\\Omega)\n",
    "$$\n",
    "所以，$\\int_{\\Omega} \\cdot v dx$ 是$H_{0}^{1}(\\Omega)$上的有界线性泛函，由Riesz定理可知，存在唯一的$w\\in H_{0}^{1}(\\Omega)$，使得：\n",
    "$$\n",
    "\\int_{\\Omega}u \\cdot v dx=(w,v)_{\\lambda_{0}},\\quad \\forall v\\in H_{0}^{1}(\\Omega)\n",
    "$$\n",
    "定义算子 $K_{\\lambda_{0}}: L^{2}(\\Omega)\\rightarrow H_{0}^{1}(\\Omega)$ 为 $w = K_{\\lambda_{0}}u$，则有：\n",
    "$$\n",
    "\\int_{\\Omega}u \\cdot v dx=(K_{\\lambda_{0}}u,v)_{\\lambda_{0}},\\quad \\forall v\\in H_{0}^{1}(\\Omega)\n",
    "$$\n",
    "并且，$\\|K_{\\lambda_{0}}u\\|_{e} \\leq c\\|u\\|$，其中$c$是一个常数。由此可知，$K_{\\lambda_{0}}$是有界(连续)线性算子。\n",
    "记 $\\iota$为$H_{0}^{1}(\\Omega) \\rightarrow L^{2}(\\Omega)$的嵌入算子，由Rellich定理，$\\iota$是紧算子。所以，原方程化为：\n",
    "$$\n",
    "(u,v)_{\\lambda_{0}}=(K_{\\lambda_{0}}\\iota u,v)_{\\lambda_{0}}+\\lambda_{0}(K_{\\lambda_{0}}\\iota u,v)_{\\lambda_{0}},\\quad \\forall v\\in H_{0}^{1}(\\Omega)\n",
    "$$\n",
    "等价于：\n",
    "$$\n",
    "(I-(\\lambda+\\lambda_{0})K_{\\lambda_{0}}\\iota)u=0\n",
    "$$\n",
    "利用紧算子的Riesz-Schauder理论以及Hilbert-Schimidt定理，我们知道$\\sigma(K_{\\lambda_{0}}\\iota ) \\setminus \\{0\\}$ 是实的，而且至多有可数个。记它们为$\\mu_{j}$,又若$\\{\\mu_{j}\\}$有可数多个不同的值，则$\\mu_{j}\\rightarrow 0(j\\rightarrow \\infty)$。因为$(K_{\\lambda_{0}}\\iota u,u)_{\\lambda_{0}}>0$，所以$\\mu_{j}>0$。又由于$H_{0}^{1}(\\Omega)$，不难验证$\\{\\mu_{j}\\}$有可数多个且$\\mu_{j}\\rightarrow 0(j\\rightarrow \\infty)$。不妨设\n",
    "$$\n",
    "\\mu_{1}\\geq\\mu_{2}\\geq\\cdots\\geq\\mu_{n}\\geq\\cdots>0\n",
    "$$\n",
    "于是特征方程的特征值为：\n",
    "$$\n",
    "\\lambda_{j}=-\\lambda_{0}+\\frac{1}{\\mu_{j}},\\quad j=1,2,\\cdots\n",
    "$$\n",
    "满足$\\lambda_{1}\\leq\\lambda_{2}\\leq\\cdots\\leq\\lambda_{n}\\leq\\cdots$以及$\\lambda_{j}\\rightarrow \\infty$。并且它们对应的特征函数构成空间 $H_{0}^{1}$ 的完备正交集。\n",
    "最后给出$\\lambda_{j}$的极小极大描写，因为\n",
    "$$\n",
    "\\mu_{j} = inf_{E_{j-1}} sup_{u\\in E^{\\perp}_{j-1},u\\neq 0}\\frac{(K_{\\lambda_{0}}\\iota u,u)_{\\lambda_{0}}}{\\|u\\|_{e}^{2}}\n",
    "$$\n",
    "其中，$E_{j-1}$是$H_{0}^{1}(\\Omega)$的$j-1$维闭线性子空间，$E^{\\perp}_{j-1}$是$E_{j-1}$的正交补。所以，$\\lambda_{j}$是下面的极小极大问题的解：\n",
    "$$\n",
    "\\lambda_{j} = \\underset{E_{j-1}}{sup}\\underset{\\underset{u\\in E^{\\perp}_{j-1}}{u\\neq 0}}{inf}\\frac{(K_{\\lambda_{0}}\\iota u,u)_{\\lambda_{0}}}{\\|u\\|_{e}^{2}}-\\lambda_{0}\\\\\n",
    "= \\underset{E_{j-1}}{sup} \\underset{\\underset{u\\in E^{\\perp}_{j-1}}{u\\neq 0}}{inf}\\frac{\\int_{\\Omega}(\\|\\nabla u\\|^{2}+Uu^{2})dx}{\\int_{\\Omega}u^{2}dx}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、数值方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 求导矩阵\n",
    "我们知道对于微分方程的数值求解，我们可以取一组采样点，再取各种差分格式，从而得到一个线性方程组，从而求解微分方程。对于特征值问题，我们可以将格式对应的求导矩阵写出，然后将微分方程的特征值问题转化为求解矩阵的特征值。下面以Laplace特征值问题为例。<br>\n",
    "为方便起见，我们首先来看一个一维的Laplace特征值问题：\n",
    "$$\n",
    "\\begin{cases}\n",
    "u''+\\lambda u=0, \\quad x\\in[0,1]\\\\\n",
    "u(0)=u(1)=0\n",
    "\\end{cases}\n",
    "$$\n",
    "假设我们对区间$[0,1]$取$N+1$个采样点，即$x_i = \\frac{i}{N}, i=0,1,\\cdots,N$，并使用中心差分格式离散，可以得到离散的微分方程：\n",
    "$$\n",
    "\\frac{u_{i+1}-2u_i+u_{i-1}}{h^2}+\\lambda u_i=0, \\quad i=1,2,\\cdots,N-1\n",
    "$$\n",
    "其中，$h=\\frac{1}{N}$。将上式写成矩阵形式，即：\n",
    "$$\n",
    "\\frac{1}{h^{2}}\n",
    "\\begin{bmatrix}\n",
    "-2 & 1 & 0 & \\cdots & 0\\\\\n",
    "1 & -2 & 1 & \\cdots & 0\\\\\n",
    "0 & 1 & -2 & \\cdots & 0\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & 0 & \\cdots & -2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "u_{1}\\\\\n",
    "u_{2}\\\\\n",
    "u_{3}\\\\\n",
    "\\vdots\\\\\n",
    "u_{N-1}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "-\\lambda\n",
    "\\begin{bmatrix}\n",
    "u_{1}\\\\\n",
    "u_{2}\\\\\n",
    "u_{3}\\\\\n",
    "\\vdots\\\\\n",
    "u_{N-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "写成矩阵形式：\n",
    "$$\n",
    "D_{N}u=-\\lambda u\n",
    "$$\n",
    "对于求导矩阵$D_N$，我们算出它的特征值和特征函数：\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\lambda_{k}=-\\frac{4}{h^{2}}\\sin^{2}\\left(\\frac{k\\pi}{2N}\\right), \\quad k=1,2,\\cdots,N-1\\\\\n",
    "    u_{k}(j)=\\sin\\left(\\frac{k\\pi j}{N}\\right), \\quad k,j=1,2,\\cdots,N-1\n",
    "\\end{cases}\n",
    "$$\n",
    "在$k<<N$时，$\\lambda_{k} \\sim \\frac{k^2\\pi^2}{N^2h^2}=k^{2}\\pi^{2}$\n",
    "所以，这种方法可以求解出特征值，但是对于特征值较大的情况，精度不高。\n",
    "如果使用**谱方法**，亲测对于这个问题大概能准确的求解出前 $0.6N$ 个特征值，如下图所示：\n",
    "![](img/Laplace1D_eigenvalue.png)\n",
    "在这里，我们没有讨论求解矩阵特征值的数值方法。当然，我们也许可以停在这里，在将算子离散为有限维的矩阵后，再结合神经网络去求矩阵的特征值。但是在这里我们不详细叙述这些方法，感兴趣的同学可以参考下面的一些文献：\n",
    "- [1] [Zhang Yi, Yan Fu, and Hua Jin Tang. Neural networks based approach for computing eigenvectors and eigenvalues of symmetric matrix. Computers and Mathematics with Applications, 47(8):1155–1164, 2004.](https://www.sciencedirect.com/science/article/pii/S0898122104901101) 使用循环神经网络来求解对称矩阵的最大和最小特征值\n",
    "- [2] [Li Zhou, Lihao Yan, Mark A. Caprio, Weiguo Gao, and Chao Yang. Solving the ksparse eigenvalue problem with reinforcement learning, 2020.](https://arxiv.org/abs/2009.04414) 强化学习求解k-稀疏特征值问题\n",
    "- [3] [Ian Gemp, Brian McWilliams, Claire Vernade, and Thore Graepel. Eigengame: {PCA} as a nash equilibrium. In International Conference on Learning Representations, 2021.](https://arxiv.org/abs/2010.00554)将PCA问题重新表述为 Eigengame，一种竞争性多代理博弈。每个玩家控制一个特征向量，他们会因为与其他玩家过于相似而受到惩罚，而他们解释数据差异的越好，他们的奖励函数的得分就越高。该博弈的纳什均衡给出了PCA的解，可以通过对每个独立玩家使用梯度下降来优化自己的奖励函数来找到它。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于将算子离散为有限维的矩阵后，对于特征值的影响，应该需要深入的泛函分析和数值分析知识，希望有大佬能够指点一二。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Deep Ritz Method\n",
    "这种方法的思路是通过将特征值问题表示为一个变分问题，然后使用神经网络来求解变分问题。下面仍然以Laplace特征值问题为例，来介绍这种方法。\n",
    "在上文的推导中，我们知道，对于Laplace的最小特征值，有如下极大极小刻画：\n",
    "$$\n",
    "\\lambda_{1} = \\underset{u\\in H^{1}_{0},u\\neq 0}{min}\\frac{\\int_{\\Omega}(\\|\\nabla u\\|^{2}+Uu^{2})dx}{\\int_{\\Omega}u^{2}dx}\n",
    "$$\n",
    "我们可以将上式写成一个约束变分问题：\n",
    "$$\n",
    "\\begin{cases}\n",
    "I(u) = \\int_{\\Omega}(\\|\\nabla u\\|^{2}+Uu^{2})dx\\rightarrow extreme\\\\\n",
    "\\int_{\\Omega}u^{2}dx=1\\\\\n",
    "u\\in H^{1}_{0}\n",
    "\\end{cases}\n",
    "$$\n",
    "上述约束变分问题的解可以通过Lagrange乘子法求解，对应的拉格朗日乘子即为最小特征值。\n",
    "Deep Ritz Method的基本思想非常简单，既然神经网络能够拟合任意的函数，不妨将$u$用神经网络 $u(x;\\theta)$ 表示，其中$\\theta$是神经网络的参数。<br>\n",
    "此外，为了避免$u$偏向于平凡解并施加约束，我们可以加入一个惩罚项，即$\\left(\\int_{\\Omega}u^{2}dx-1\\right)^{2}$,最终的损失函数为\n",
    "$$\n",
    "L(u(x;\\theta)) = \\frac{\\int_{\\Omega}(\\|\\nabla u\\|^{2}+Uu^{2})dx}{\\int_{\\Omega}u^{2}dx}+\\alpha\\left(\\int_{\\Omega}u^{2}dx-1\\right)^{2}\n",
    "$$\n",
    "通过随机梯度下降法，我们可以求解上述损失函数的最小值，从而得到最小特征值。\n",
    "对于Sturm-Liouville问题（本质边界条件，记号同问题描述中所示），我们同样可以将其写成一个约束变分问题，最终的损失函数为：\n",
    "$$\n",
    "L(u(x;\\theta)) = \\frac{\\int_{a}^{b}\\left(p(x)\\left(\\frac{du}{dx}\\right)^{2}-q(x)u^{2}\\right)dx}{\\int_{a}^{b}w(x)u^{2}dx}+\\alpha\\left(\\int_{a}^{b}w(x)u^{2}dx-1\\right)^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Ritz Method的拟合网络结构十分简单，一个DNN搞定~\n",
    "![DNN-like](img/DeepRitz_DNN_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Ritz Method的优点:\n",
    "- 它对问题的维度不太敏感，并且有可能在相当高的维度上工作。在原始论文中，作者给出来$d=1,5,10$维的计算结果。\n",
    "- 该方法相当简单，非常适合深度学习中常用的随机梯度下降框架\n",
    "\n",
    "\n",
    "Deep Ritz Method的缺点:\n",
    "- 即使最初的问题是凸的，我们最终得到的变分问题也不是凸的。局部极小值和鞍点的问题并非无关紧要。\n",
    "- 目前，对于收敛速度还没有一致的结论。\n",
    "- 本质边界条件的处理并不像传统方法那么简单。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文献：\n",
    "- [1] [E W, Yu B. The deep Ritz method: a deep learning-based numerical algorithm for solving variational problems. Commun Math Stat 2018;6(1):1–12.](https://link.springer.com/article/10.1007/s40304-018-0127-z)\n",
    "- [2] [Alexander K. Conditional physics informed neural networks](https://sci-hub.ru//uptodate/S1007570421003531.pdf#navpanes=0&view=FitH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Deep Backward SDE Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑特征值问题：\n",
    "$$\n",
    "L u = \\lambda u\n",
    "$$\n",
    "定义域$\\Omega = [0,2\\pi]^{d}$,$u$ 满足周期边界条件，$L$是一个线性算子且满足如下形式：\n",
    "$$\n",
    "Lu(x) = -\\frac{1}{2}Tr\\left(\\sigma\\sigma^{T}Hess(u)(x)\\right)-b(x)\\cdot \\nabla u(x) + f(x)u(x)\n",
    "$$\n",
    "其中，$\\sigma$是一个$d\\times d$的常值矩阵，$b(x)$是一个$d$维向量，$f(x)$是一个标量函数。\n",
    "现在，我们考虑如下的反向抛物偏微分方程：\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\partial_{t}u(t,x)-Lu(t,x) + \\lambda u(t,x) = 0, \\quad in [0,T]\\times \\Omega \\\\\n",
    "u(T,x) = \\Psi(x), \\quad x\\in \\Omega\n",
    "\\end{cases}\n",
    "$$\n",
    "**这本质上是连续版的矩阵特征值问题的幂法**。记抛物PDE的解为$u(T-t,x) = P^{\\lambda}_{t}\\Psi$，根据椭圆算子的谱理论，如果$\\Psi$是PDE的一个稳态解，也就是说，$P^{\\lambda}_{T}\\Psi = \\Psi$，则$(\\lambda,\\Psi)$是原方程的一个特征对。自然地，我们可以用$\\|P^{\\lambda}_{T}-\\Psi\\|^{2}$作为loss function，虽然这是一个非凸优化问题，但我们期望通过适当的初始化局部收敛到有效的特征对。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们已经将特征值问题转化为求解一个抛物型偏微分方程的问题。对于后者，我们可以用Deep BSDE Method来求解。设 $X_{t}$是随机微分方程的解，即：\n",
    "$$\n",
    "dX_{t} =\\sigma dW_{t}\n",
    "$$\n",
    "其中，$W_{t}$是$d$维Brown运动。我们可以将$X_{t}$的解写成如下积分形式：\n",
    "$$\n",
    "X_{t} = X_{0} + \\int_{0}^{t}\\sigma dW_{s}\n",
    "$$\n",
    "其中，$X_{0}$是一个从初始分布$\\upsilon$中采样的随机变量，通过Ito's formula，我们可以得到,PDE的解$u(t,x)$满足：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "u(t,X_{t}) &= u(0,X_{0}) + \\int_{0}^{t}Lu(s,X_{s})ds + \\int_{0}^{t}\\sigma^{T}\\nabla u(s,X_{s})dW_{s} \\\\\n",
    "&= u(0,X_{0}) + \\int_{0}^{t}\\left(f(X_{s})u(s,X_{s})-\\lambda u(s,X_{s})-b(X_{s})\\nabla u(s,X_{s})\\right)ds + \\int_{0}^{t}\\sigma^{T}\\nabla u(s,X_{s})dW_{s}\n",
    "\\end{aligned}\n",
    "$$\n",
    "上式中$u(s,\\cdot),\\nabla u(s,\\cdot)$的先验还不知道，但我们知道如果$u(s,\\cdot) = \\Psi(\\cdot)$（即我们要求的特征函数），$\\nabla u(s,\\cdot) = \\nabla \\Psi(\\cdot)$，则$u(t,\\cdot)=\\Psi(\\cdot)$对于任意$t$成立。所以，我们可以用两个神经网络$\\mathfrak{R}_{\\Psi}$,$\\mathfrak{R}_{\\sigma^{T}\\nabla \\Psi}$来近似$u(s,\\cdot),\\nabla u(s,\\cdot)$，那么，损失函数可以用$t=T$时的误差来表示：\n",
    "$$\n",
    "L(\\mathfrak{R}_{\\Psi},\\mathfrak{R}_{\\sigma^{T}\\nabla \\Psi})= \\mathbb{E}_{X_{0}\\sim \\upsilon}\\left[\\eta_{1}\\left\\|\\mathfrak{R}_{\\Psi}(X_{T})-u(T,X_{T})\\right\\|^{2}+\\eta_{2}\\left\\|\\mathfrak{R}_{\\sigma^{T}\\nabla \\Psi}(X_{T}^{k})-\\sigma^{T}\\nabla\\mathfrak{R}_{\\Psi}(X_{T})\\right\\|^{2}\\right]\n",
    "$$\n",
    "下面我们将通过Euler-Mayamura法和Monto Carlo方法求上述loss，从而优化$(\\lambda,\\mathfrak{R}_{\\Psi},\\mathfrak{R}_{\\sigma^{T}\\nabla \\Psi})$。\n",
    "离散时间域$[0,T]$，将其分成$N$个时间段，记$0=t_{0}<t_{1}<\\cdots<t_{N}=T$，其中$t_{n+1}-t_{n}=\\Delta t_{n}$。我们可以用Euler-Mayamura法来近似$X_{t}$，即：\n",
    "$$\n",
    "x_{0}\\sim \\upsilon,\\quad x_{t_{n+1}} = x_{t_{n}} + \\sigma\\Delta W_{n}\\\\\n",
    "u_{0} \\approx \\mathfrak{R}_{\\Psi}(x_{0}),\\quad u_{t_{n+1}} = u_{t_{n}} + \\left(f(x_{t_{n}})u_{t_{n}}-\\lambda u_{t_{n}}-b(x_{t_{n}})\\nabla u_{t_{n}}\\right)\\Delta t_{n} + \\sigma^{T}\\nabla u_{t_{n}}\\Delta W_{n}\\\\\n",
    "\\quad \\quad \\quad \\quad \\quad \\quad \\quad\\approx  u_{t_{n}} + \\left(f(x_{t_{n}})u_{t_{n}}-\\lambda u_{t_{n}}-b(x_{t_{n}})\\sigma^{-T}\\mathfrak{R}_{\\sigma^{T}\\nabla \\Psi}(x_{t_{n}})\\right)\\Delta t_{n} + \\mathfrak{R}_{\\sigma^{T}\\nabla \\Psi}(x_{t_{n}})\\Delta W_{n}\n",
    "$$\n",
    "其中，小写的字母表示对应的离散值，$\\Delta W_{n} = W_{t_{n+1}}-W_{t_{n}}$，离散形式的loss function:\n",
    "$$\n",
    "L(\\mathfrak{R}_{\\Psi},\\mathfrak{R}_{\\sigma^{T}\\nabla \\Psi})= \\mathbb{E}_{X_{0}\\sim \\upsilon}\\left[\\eta_{1}\\left\\|\\mathfrak{R}_{\\Psi}(x_{T})-u_{T}\\right\\|^{2}+\\eta_{2}\\left\\|\\mathfrak{R}_{\\sigma^{T}\\nabla \\Psi}(x_{T}^{k})-\\sigma^{T}\\nabla\\mathfrak{R}_{\\Psi}(x_{T})\\right\\|^{2}\\right]\n",
    "$$\n",
    "Monto-Carlo方法近似上述期望：\n",
    "取$K$个样本$x_{0}^{k}\\sim \\upsilon$，$k=1,2,\\cdots,K$，则期望近似为：\n",
    "$$\n",
    "\\frac{1}{K}\\sum_{k=1}^{K}\\left[\\eta_{1}\\left\\|\\mathfrak{R}_{\\Psi}(x_{T}^{k})-u_{T}^{k}\\right\\|^{2}+\\eta_{2}\\left\\|\\mathfrak{R}_{\\sigma^{T}\\nabla \\Psi}(x_{T}^{k})-\\sigma^{T}\\nabla\\mathfrak{R}_{\\Psi}(x_{T}^{k})\\right\\|^{2}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**归一化**：为了防止平凡解$(\\cdot,\\mathfrak{R}_{\\Psi}\\equiv0,\\mathfrak{R}_{\\sigma^{T}\\nabla \\Psi}\\equiv 0)$，我们需要对其进行归一化，即：\n",
    "$$\n",
    "\\int_{\\Omega}\\mathfrak{R}_{\\Psi}^{2}(x)dx = |\\Omega|\n",
    "$$\n",
    "引入归一化常数$Z_{\\Psi}$:\n",
    "$$\n",
    "Z_{\\Psi} = sign(\\int_{\\Omega}\\mathfrak{R}_{\\Psi}(x)dx)\\left(\\int_{\\Omega}\\mathfrak{R}_{\\Psi}^{2}(x)dx\\right)^{\\frac{1}{2}}\n",
    "$$\n",
    "然而，对于$Z_{\\Psi}$我们无法直接从神经网络中得到，所以我们需要对其进行近似。我们可以第$l$步的训练中的$K$个采样来近似$Z_{\\Psi}$：\n",
    "$$\n",
    "\\hat{Z}^{l}_{\\Psi} = sign(\\sum_{k=1}^{K}\\mathfrak{R}_{\\Psi}(x_{T}^{k,l}))\\left(\\frac{1}{K}\\sum_{k=1}^{K}\\mathfrak{R}_{\\Psi}^{2}(x_{T}^{k,l})\\right)^{\\frac{1}{2}}\n",
    "$$\n",
    "然而，由于$\\mathbb{E}(\\frac{A}{B})\\neq \\frac{\\mathbb{E}A}{\\mathbb{E}B} $，在Loss function中引入基于样本均值近似的归一化常数将导致对期望的蒙特卡洛近似有偏，从而导致收敛到错误的解。为了解决这个问题，我们可以使用移动平均法(moving average)来减小由本批次样本计算的归一化常数所引起的偏差，即：\n",
    "$$\n",
    "Z_{\\Psi}^{l} = \\gamma_{l} Z_{\\Psi}^{l-1} + (1-\\gamma_{l})\\hat{Z}_{\\Psi}^{l}\n",
    "$$\n",
    "这里$\\gamma_{l}$是衰减的移动平均系数。可以看出，一开始较小的$\\gamma_{l}$可以提高训练效率，随后增大其值，使得梯度偏差较小。\n",
    "最终，我们得到了如下的loss function:\n",
    "$$\n",
    "L(\\mathfrak{R}_{\\Psi},\\mathfrak{R}_{\\sigma^{T}\\nabla \\Psi})= \\mathbb{E}_{X_{0}\\sim \\upsilon}\\left[\\eta_{1}\\left\\|\\frac{\\mathfrak{R}_{\\Psi}(X_{T})}{\\|\\mathfrak{R}_{\\Psi}\\|_{2}/|\\Omega|^{\\frac{1}{2}}}-u(T,X_{T})\\right\\|^{2}+\\eta_{2}\\left\\|\\mathfrak{R}_{\\sigma^{T}\\nabla \\Psi}(X_{T})-\\frac{\\sigma^{T}\\nabla\\mathfrak{R}_{\\Psi}(X_{T})}{\\|\\mathfrak{R}_{\\Psi}\\|_{2}/|\\Omega|^{\\frac{1}{2}}}\\right\\|^{2}\\right]\\\\\n",
    "\\approx \\frac{1}{K}\\sum_{k=1}^{K}\\left[\\eta_{1}\\left\\|\\frac{\\mathfrak{R}_{\\Psi}(x_{T}^{k})}{Z^{l}_{\\Psi}}-u_{T}^{k}\\right\\|^{2}+\\eta_{2}\\left\\|\\mathfrak{R}_{\\sigma^{T}\\nabla \\Psi}(x_{T}^{k})-\\frac{\\sigma^{T}\\nabla\\mathfrak{R}_{\\Psi}(x_{T}^{k})}{Z^{l}_{\\Psi}}\\right\\|^{2}\\right] + \\eta_{3}(Z_{0}-Z^{l}_{\\Psi})^{+}\n",
    "$$\n",
    "新增的一项$\\eta_{3}(Z_{0}-Z^{l}_{\\Psi})^{+}$中$\\Z_{0}$是一个超参数，$\\eta_{3}$是对应的权重，目的是防止$Z^{l}_{\\Psi}$过小，从而导致训练不稳定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep BSDE算法与幂法的联系**：两种算法都寻求在传播下稳定的解决方案。然而，一个区别是Deep BSDE也可以用于一般特征值，具体取决于 $\\lambda$ 和 $\\Psi$ 的初始化。使用矩阵表示法，这类似于使用目标函数$\\|(A-\\lambda)\\upsilon\\|^{2}$ 来查找矩阵$A$的非主导特征对$(\\lambda,\\upsilon)$。求解非主导特征值的实际性能取决于初始化和特征值的分布，原论文作者在数值实验中发现，如果 $\\lambda$ 初始化得足够小，它总是会收敛到第一个特征值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络结构：\n",
    "![DeepBSDE_arch](img/DeepBSDE_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文献：\n",
    "- [1] [Solving high-dimensional eigenvalue problems using deep neural networks: A diffusion Monte Carlo like approach](https://arxiv.org/abs/2002.02600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 其它方法\n",
    "这里我就只给出参考文献了，感兴趣的同学可以自行阅读：\n",
    "- [1] [abumosameh-pythoncodeforsturmliouvilleeigenvalueproblems](https://repository.library.carleton.ca/concern/etds/4t64gp05w)\n",
    "- [2] [Physics-Informed Neural Networks for Quantum Eigenvalue Problems](https://arxiv.org/abs/2203.00451)这篇太残暴了。。。\n",
    "- [3] [Application of machine learning regression models to inverse eigenvalue problems](https://www.sciencedirect.com/science/article/abs/pii/S089812212300545X)特征值问题的反问题\n",
    "- [4] [Solving eigenvalueproblems with Neural Network](https://mediatum.ub.tum.de/doc/1632870/kwm0n4od0og42tg17pewgnx5s.pdf)一篇硕士论文，用Spectralnet 解了一个Heat equation的特征值问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
