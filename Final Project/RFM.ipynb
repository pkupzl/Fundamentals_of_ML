{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RFM | 随机特征方法 Random Feature Method","metadata":{},"id":"e932e0e7-fa83-41ed-961d-826a3057ed41"},{"cell_type":"markdown","source":"<div style=\"color:black; background-color:#FFF3E9; border: 1px solid #FFE0C3; border-radius: 10px; margin-bottom:0rem\">\n    <p style=\"margin:1rem; padding-left: 1rem; line-height: 2.5;\">\n        ©️ <b><i>Copyright 2023 @ Authors</i></b><br/>\n        <i>作者：\n            <b>\n            <a href=\"mailto:cxr123@mail.ustc.edu.cn\">池煦荣 📨 </a>\n            </b>\n        </i>\n        <br/>\n        <i>日期：2023-07-06</i><br/>\n        <i>共享协议：</a>本作品采用<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。</i><br/>\n        <i>快速开始：点击上方的</i> <span style=\"background-color:rgb(85, 91, 228); color:white; padding: 3px; border-radius: 5px;box-shadow: 2px 2px 3px rgba(0, 0, 0, 0.3); font-size:0.75rem;\">开始连接</span> <i>按钮，选择 <b><u>bohrium-notebook:2023-05-31</u>镜像</b> 和任意配置机型即可开始。\n    </p>\n</div>","metadata":{},"id":"5bfb5569-64f6-4ff1-a444-dd2936204e9e"},{"cell_type":"markdown","source":"## 背景介绍\n微分方程数值方法在当下主流的科学研究与工业应用中都发挥着重要作用：在科学研究领域，高维的随机控制问题本质是求解高维Bellman方程；从头算分子动力学则结合了密度泛函理论和分子动力学技术，目标也是求解描述电子状态的Kohn-Sham方程和描述离子状态的Newton方程。在工业应用领域，计算机辅助工程（CAE）系统通过计算机软件对性能进行仿真分析，证实未来工程和产品功能的可用性、性能的可靠性，从而改善产品设计或协助解决各个行业的工程问题。基于微分方程表示的物理定律构成了CAE软件的基础，包括固体力学方程、流体力学方程和电磁学方程等等。\n因此，微分方程数值方法是科学计算研究领域最重要的课题之一，同时它也是工业CAE软件的关键组成部分。\n\n在这样的大背景下，许多微分方程数值方法被提出并广泛研究。其中经典的微分方程数值方法通常具有稳定的收敛阶，但依赖于网格离散的特性让它们难以处理复杂几何上的问题；而机器学习方法在这类复杂问题上具有优势，但求解误差不可控，且相比于经典方法需要很长的求解时间。作为一种新型的微分方程数值方法，本文的主角：随机特征方法（The random feature method[<sup>5</sup>](#R5), RFM）融合了经典方法和机器学习方法的优势，是一种兼具稳定收敛性和简便性的新型微分方程数值方法。\n\n作为一个典型应用的例子，在本文中我们将考虑Helmholtz方程的求解。在电磁场问题中，由无源区的Maxwell方程组可推导出Wave equation，再经由Fourier变换就可得到如下的Helmholtz方程\n\n\\begin{equation*}\n\t\t\\left\\{\\begin {aligned}\n\t\t(\\Delta + \\lambda^2) u(\\boldsymbol{x}) &= f(\\boldsymbol{x}) \\quad && \\boldsymbol{x}\\in \\Omega\\\\\n\t\tu(\\boldsymbol{x})  &= U(\\boldsymbol{x}) \\quad && \\boldsymbol{x} \\in \\partial\\Omega\n\t\t\\end {aligned}\\right.\n\t\t%\\label{stokesflow}\n\t\\end{equation*}\n\n在正式开始之前，我们将三类方法的基本特点总结如下（<font color=\"Red\">红色</font>代表优势、<font color=\"Blue\">蓝色</font>代表劣势，下同），后续将通过正文中的实验或讨论进一步阐述\n1. 经典数值方法：\n    - 线性系统往往行列数相等（条件个数=自由度个数）\n    - <font color=\"Red\">通常具有稳定的收敛阶（在log-log误差图中误差线性下降）</font>\n    - <font color=\"Blue\">依赖网格，难以处理复杂的计算区域</font>\n    - <font color=\"Blue\">无法求解高维问题（存在维数灾难）</font>\n    - <font color=\"Blue\">无法求解反问题（不可微分框架）</font>\n1. 机器学习方法：\n    - 线性系统行列数可以不相等（条件个数≠自由度个数）\n    - <font color=\"Red\">不依赖网格，在处理复杂区域时有显著优势</font>\n    - <font color=\"Red\">能够求解极其高维的微分方程，甚至可以用于解算子的参数化</font>\n    - <font color=\"Red\">可微分框架，能够在同一框架下求解反问题</font>\n    - <font color=\"Blue\">需要长时训练，方程求解时间长</font>\n    - <font color=\"Blue\">非凸优化导致方程数值解的精度有限，无法系统性优化</font>\n    - <font color=\"Blue\">边界罚项中罚参数的调整困难</font>\n1. 随机特征方法（RFM）：\n    - 线性系统行列数可以不相等（条件个数≠自由度个数）\n    - <font color=\"Red\">具有谱精度（在semi-log误差图中误差线性下降）</font>\n    - <font color=\"Red\">不依赖网格，在处理复杂区域时有显著优势</font>\n    - <font color=\"Red\">线性最小二乘优化框架，求解精度高、效率高</font>\n    - <font color=\"Red\">可微分框架，能够在同一框架下求解反问题</font>\n    - <font color=\"Red\">边界罚项中罚参数的调整容易</font>\n\n在本Notebook中，我们将以上述Helmholtz方程的求解为例，给出可运行的代码，在同一实验设置下（线性系统规模相等）对三类方法进行对比与讨论。\n","metadata":{},"id":"03b862ce-78ed-47d7-818f-b9380938cfd7"},{"cell_type":"markdown","source":"## 目录\n\n* [1. 依赖于离散的经典数值方法：以有限差分方法（FDM）为代表](#layer0)\n* [2. 基于机器学习模型的微分方程数值方法：以物理信息神经网络（PINN）为代表](#layer1)\n* [3. 博采众长的随机特征方法：The random feature method （RFM）](#layer2)\n\n\n阅读本文可能需要30分钟以上，边运行边消化可能需要**1-2小时**。","metadata":{},"id":"fd20f898-4365-41bc-a84e-fc11bcb2c8aa"},{"cell_type":"markdown","source":"## 依赖于离散的经典数值方法 <a id='layer0'></a>\n\n\n### 不同方程形式，不同的离散方式\n经典的微分方程数值方法通常需要将数学模型离散化、得到相应的数值模型，再求解离散后的方程。但值得注意的是，离散的细节根据所求解方程的形式的不同会有所不同：\n- 方程强形式是我们最熟悉的方程形式，对于一维Helmholtz方程来说，它的强形式就是下式\n\\begin{equation*}\n\t\t\\left\\{\\begin {aligned}\n\t\t(\\Delta + \\lambda^2) u(\\boldsymbol{x}) &= f(\\boldsymbol{x}) \\quad && \\boldsymbol{x}\\in \\Omega\\\\\n\t\tu(\\boldsymbol{x})  &= U(\\boldsymbol{x}) \\quad && \\boldsymbol{x} \\in \\partial\\Omega\n\t\t\\end {aligned}\\right.\n\t\t%\\label{stokesflow}\n\t\\end{equation*}\n基于方程强形式的方法在离散时往往会先进行方程的离散，在网格点（grid points）或称为配点（collocation points）上施加方程$$(\\Delta + \\lambda^2) u(x) = f(x)\\quad\\rightarrow\\quad (\\Delta + \\lambda^2) u(x_i) = f(x_i)$$\n此外，有限差分方法需要额外对算子进行离散。\n- 方程的弱形式是强形式通过分部积分得到的，在有限维解空间中进行近似。一维Helmholtz方程的弱形式可表为：求解 $u \\in H_0^1(\\Omega)$，使得\n$$\n- \\int_{\\Omega} \\nabla u \\cdot \\nabla v \\mathrm{~d} V  + \\lambda^2 \\int_{\\Omega}  u \\cdot v \\mathrm{~d} V=\\int_{\\Omega} f v \\mathrm{~d} V, \\quad \\forall v \\in H_0^1(\\Omega)\n$$\n因此基于弱形式的方法依赖于数值积分，需要利用被积函数在有限个离散点上的函数值来计算积分近似值。**在实际实现过程中，许多方法（有限元法、谱元法等）在计算时需要先进行网格生成（如下图），这可能占据整个计算过程的70%以上的时间；此外，在几何非常复杂的情况下甚至无法进行网格生成！**\n\n<img src=\"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17477/725632357018477f98642a991fe539e0/363598cf-5e66-42ef-96f6-6cec131bd9f2.png\"  width=\"200\" alt=\"二维复杂区域的有限元网格示例\">\n\n- 方程的变分形式将方程求解转化为最小化势能函数的优化问题，但同样依赖于数值积分，在此不再赘述。\n\n不管以上这些经典数值方法基于方程的哪种形式（强形式/弱形式/变分形式），<font color=\"Red\">它们通常都具有稳定的收敛阶和完备的收敛理论</font>。但对于计算区域几何复杂的情形，<font color=\"Blue\">网格生成困难且耗时</font>；此外，由于维度灾难的存在，这类数值方法<font color=\"Blue\">基本无法求解高维问题</font>。","metadata":{},"id":"3f337905-7b88-49fc-835e-208d44cb51e2"},{"cell_type":"markdown","source":"### 基于方程强形式的有限差分方法\n\n有限差分方法是基于大家最熟悉的方程强形式的数值方法，此外，离散思想在有限差分方法的各个步骤中都有所体现，甚至其最终计算结果也是在离散点上的输出。因此我们以有限差分方法为例来介绍经典方法是如何进行离散的\n1. 方程的离散：如上所述，基于强形式的有限差分方法的方程离散通过在网格点（配点）上施加方程来实现，即\n$$(\\Delta + \\lambda^2) u(x_i) = f(x_i), \\quad n=1,\\cdots,N-1$$\n1. 微分算子的离散：可以看到，在上面的离散方程中，Laplace算子 $\\Delta$ 作用下的 $\\Delta  u(x_i)$ 在求解函数值 $u^h_i = u(x_i)$ 时并不是可以直接得到的，因此考虑对Laplace算子用如下中心差分的离散格式来取代\n$$\\Delta  u(x_i) = \\frac{u^h_{i+1} - 2 u^h_{i} + u^h_{i-1}}{h^2}=f^h_{i}$$\n1. 边界条件的离散：对于边界条件 $u(x)  = U(x)$，它的离散是容易处理的，我们只需在边界的网格点上计算\n$$u^h_0 = u(x_0)=U(x_0), \\quad u^h_{N}=u(x_N)=U(x_N)$$\n因此端点上的函数值我们可以直接得到，不需要纳入线性系统的求解过程。\n1. 通过上面的三个步骤，我们实际上完成了矩阵装配、获得了待求解的线性系统\n\\begin{equation*}\n\t\t\\frac{1}{h^2}\\left[\\begin{array}{ccccc}\n\t\t\th^2\\lambda^2-2 & 1 & & & \\\\\n\t\t\t1 & h^2\\lambda^2-2 & 1 & & \\\\\n\t\t\t& \\cdots & \\cdots & \\cdots & \\\\\n\t\t\t& & 1 & h^2\\lambda^2-2 & 1 \\\\\n\t\t\t& & & 1 & h^2\\lambda^2-2\n\t\t\\end{array}\\right]\\left[\\begin{array}{c}\n\t\t\tu_1^h \\\\\n\t\t\tu_2^h \\\\\n\t\t\t\\cdots \\\\\n\t\t\tu_{N-2}^h \\\\\n\t\t\tu_{N-1}^h\n\t\t\\end{array}\\right]=\n\t\t\\left[\\begin{array}{l}\n\t\t\tf_1^h+u_0^h / h^2 \\\\\n\t\t\tf_2^h \\\\\n\t\t\t\\cdots \\\\\n\t\t\tf_{N-2}^h \\\\\n\t\t\tf_{N-1}^h+u_N^h / h^2\n\t\t\\end{array}\\right]\n\t\\end{equation*}\n求解上述三对角线性系统 $Au=f$，我们就能获得所有配点 $x_i$ 上的函数值 $u^h_i = u(x_i), n=0,\\cdots,N$。\n\n\n有了上述的步骤拆解，下一小节中我们将给出有限差分方法的代码实现。","metadata":{},"id":"1f55591e-da6b-4a17-ab14-1215175373c7"},{"cell_type":"markdown","source":"### 基于Python的有限差分方法代码实现\n\n由于上一节已对有限差分法的步骤进行了详尽的描述，以下代码实现中我们将直接进行线性系统 $Au=f$ 的装配。\n\n我们首先在计算域 $[0,8]$ 上定义真解函数，它由频率不同的 $\\sin$, $\\cos$ 函数的乘积生成","metadata":{},"id":"653bfe56-2558-406e-9c8e-6cb98d083253"},{"cell_type":"code","source":"import numpy as np\nimport math\nimport time\nfrom numpy.linalg import solve\n\n# analytical solution parameters\nAA = 1\naa = 2.0*np.pi\nbb = 3.0*np.pi\n\nleft_boundary = 0.0\nright_boundary = 8.0\n\nlamb = 4\ndef u(x):\n    return AA * np.sin(bb * (x + 0.05)) * np.cos(aa * (x + 0.05)) + 2.0\n\ndef d2u_dx2(x):\n    return -AA*(aa*aa+bb*bb)*np.sin(bb*(x+0.05))*np.cos(aa*(x+0.05))\\\n           -2.0*AA*aa*bb*np.cos(bb*(x+0.05))*np.sin(aa*(x+0.05))\n\ndef f(x):\n    return(d2u_dx2(x) + lamb*u(x))\n\nvanal_u = np.vectorize(u)\nvanal_f = np.vectorize(f)","metadata":{},"execution_count":1,"outputs":[],"id":"1ae5590b-9fb3-4e0d-b2a2-eabb680b7de0"},{"cell_type":"markdown","source":"然后我们就可以直接进行线性系统 $Au=f$ 的装配，并返回矩阵 $A$ 和向量 $f$","metadata":{},"id":"86de2084-54f7-4f51-bd57-f35edb373dc4"},{"cell_type":"code","source":"# calculate the matrix A,f in linear equations system 'Au=f'\ndef cal_matrix(N, points):\n    A = np.zeros((N+1,N+1),dtype=np.float64)\n    dx = (right_boundary - left_boundary)/N\n    for i in range(1,N):\n        A[i,i-1] = 1/(dx**2)\n        A[i,i] = lamb - 2/(dx**2)\n        A[i,i+1] = 1/(dx**2)\n    A[0,0] = 1.0\n    A[N,N] = 1.0\n    f = np.zeros((N+1,1),dtype=np.float64)\n    f[0,0] = u(points[0])\n    f[1:N] = vanal_f(points[1:N]).reshape((-1,1))\n    f[N] = u(points[-1])\n    return(A,f)","metadata":{},"execution_count":2,"outputs":[],"id":"ab555944-5bbd-42b2-80d5-379ad8e38e65"},{"cell_type":"markdown","source":"最后，我们定义测试函数和绘图函数，并在线性系统规模分别为 $100,200,400,800,1600,3200$ 时进行测试，以体现有限差分法的收敛阶（在log-log误差图中误差线性下降）","metadata":{},"id":"d63d14cb-c614-4c7a-991f-bfd577a4d0e8"},{"cell_type":"code","source":"# calculate the l^{inf}-norm and l^{2}-norm error for u\ndef test(points, u):\n    true_values = vanal_u(points)\n    numerical_values = u\n    epsilon = true_values - numerical_values\n    epsilon = np.maximum(epsilon, -epsilon)\n    error_inf = epsilon.max()\n    error_l2 = math.sqrt(8*sum(epsilon*epsilon)/len(epsilon))\n    print('L_infty=',error_inf,'L_2=',error_l2)\n    return(error_l2)\n\ndef error_plot(multi_Errors):\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=[7, 5])\n    plt.tick_params(labelsize=10)\n    font2 = {\n    'weight' : 'normal',\n    'size'   : 22,\n    }\n    plt.xlabel('Degrees of freedom',font2)\n    plt.ylabel('$L_2$ absolute error',font2)\n    plt.xscale('log')\n    plt.yscale('log')\n    Label = ['FDM','PINN','RFM']\n    for i in range(len(multi_Errors)):\n        Error = multi_Errors[i]\n        plt.plot(Error[:,0], Error[:,1], \\\n                 lw=1.5, ls='-', clip_on=False,\\\n                 marker='o', markersize=10, \\\n                 label = Label[i],\\\n                 markerfacecolor='none',\\\n                 markeredgewidth=1.5)\n    plt.legend()\n    plt.title(\"Comparison of accuracy on 1D Helmholtz equation\")\n    plt.show()\n\ndef time_plot(multi_Errors):\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=[7, 5])\n    plt.tick_params(labelsize=10)\n    font2 = {\n    'weight' : 'normal',\n    'size'   : 22,\n    }\n    plt.xlabel('Degrees of freedom',font2)\n    plt.ylabel('Solving time',font2)\n    Label = ['FDM','PINN','RFM']\n    for i in range(len(multi_Errors)):\n        Error = multi_Errors[i]\n        plt.plot(Error[:,0], Error[:,2], \\\n                 lw=1.5, ls='-', clip_on=False,\\\n                 marker='o', markersize=10, \\\n                 label = Label[i],\\\n                 markerfacecolor='none',\\\n                 markeredgewidth=1.5)\n    plt.legend()\n    plt.title(\"Comparison of efficiency on 1D Helmholtz equation\")\n    plt.show()\n    \ndef main(N):\n    time_begin = time.time()\n    points = np.linspace(0, 8.0, N+1)\n    A,f = cal_matrix(N,points)\n    u = solve(A,f).reshape((-1))\n    error = test(points, u)\n    time_end = time.time()\n    return(error, time_end - time_begin)\n\nif __name__ == '__main__':\n    FDM_Error = np.zeros([5,3])\n    for i in range(5):\n        N = int(100 * 2**i)\n        print('***********************')\n        print(\"N = M =\",N)\n        FDM_Error[i,0] = N\n        FDM_Error[i,1], FDM_Error[i,2] = main(N)\n    \n    error_plot([FDM_Error])","metadata":{},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":"***********************\nN = M = 100\nL_infty= 0.4038518006880172 L_2= 0.6818204170645585\n***********************\nN = M = 200\nL_infty= 0.10009073936912527 L_2= 0.16727260903655303\n***********************\nN = M = 400\nL_infty= 0.024869758483598403 L_2= 0.04167428236543972\n***********************\nN = M = 800\nL_infty= 0.006221760357883799 L_2= 0.010413267709307231\n***********************\nN = M = 1600\nL_infty= 0.0015548590687854968 L_2= 0.0026033988607706963\n"},{"output_type":"display_data","data":{"remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17477/a00e68bfd76d41ca83c2213895abd665/8f6fc20e22d8405481a3dbcc93f29e84.png"},"metadata":{}}],"id":"1f7e21ab-11ea-48ae-b828-44432cc35f4b"},{"cell_type":"markdown","source":"### 小结（经典数值方法）\n1. 在上述log-log误差曲线中，我们可以很明显地看出线性趋势，<font color=\"Red\">这与有限差分方法的理论分析一致、说明它有着非常严格的收敛阶</font>；\n1. 另一方面，虽然在一维Helmholtz问题中并不明显，但有限差分方法依赖于规则网格点离散的特性，使得它<font color=\"Blue\">在求解复杂几何问题时会遇到困难</font>，需要引入虚拟点（如下图）并设计特殊的差分格式来解决这一问题；类似的，基于网格的有限元、谱元法等经典数值方法需要在整个求解域内生成网格，同样难以处理复杂几何上的问题；\n1. 由于有限差分方法在计算时需要将计算域离散，若对每个维度都进行 $N$ 等分的离散，则在 $d$ 维问题中，规则网格的自由度达到 $N^d$ 的指数增长，即我们常说的维度灾难问题，这也导致这类经典数值方法<font color=\"Blue\">基本无法求解高维问题</font>。\n\n<img src=\"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17477/eb548f21a3ab44db8ca4038f52b95f3d/096e10a4-c3f3-4329-ae4f-1143230c20c2.png\"  width=\"200\" alt=\"二维圆弧上有限差分方法虚拟点示例\">\n","metadata":{},"id":"166fdab6-ffc1-4e1f-8a48-006d59f5f16e"},{"cell_type":"markdown","source":"## 基于机器学习模型的微分方程数值方法 <a id='layer1'></a>\n\n基于机器学习的微分方程数值方法是近年来的新兴方法，与经典数值方法相比，机器学习方法的长处在于求解高维的PDE和控制问题，甚至可以被用来进行PDE解算子的参数化。但机器学习方法误差具有不可控性；且模型需要长时训练，因此方程求解所需时间往往较长。\n\n### 如何借助机器学习模型求解微分方程\n从以上的例子我们可以看出，经典的微分方程数值方法从大框架上看有三个主要的构成\n- 数值离散：配点（强形式）、数值积分（弱形式、变分形式）\n- 逼近空间：全局/局部基函数的线性组合\n- 优化问题：线性方程组求解\n\n注意到，在机器学习方法中，有完全类似的概念\n- 数值离散：配点、数值积分构造损失函数\n- 逼近空间：神经网络模型\n- 优化问题：非线性优化，可使用随机梯度下降（SGD）\n\n因此引入机器学习模型来求解微分方程的想法是非常自然的。且同样自然地，针对不同的方程形式，有对应的机器学习方法来求解：\n\n- 强形式: Deep Galerkin Method[<sup>1</sup>](#R1) (DGM) 和 Physics-Informed Neural Networks[<sup>2</sup>](#R2) (PINN) 在配点上定义损失函数，\n$$\nL(u)=\\left\\|(\\Delta + \\lambda^2) u - f\\right\\|_{2,\\Omega}^2+\\lambda\\|u-U\\|_{2, \\partial \\Omega}^2\n$$\n其中方程的强形式用最小二乘形式给出，再加上带罚参数的边界条件罚项来组合损失函数。\n- 弱形式: 基于方程弱形式构造的机器学习方法中，最著名的是 Weak Adversarial Network[<sup>3</sup>](#R3) (WAN) 方法，它通过借鉴生成对抗网络（GAN）思想，将弱形式的方程求解转化为$min-max$问题的优化求解。具体来说，首先定义算子 $\\mathcal{A}$ 和 $\\mathcal{B}$，将弱形式方程重写为\n\\begin{equation*}\n\t\\left\\{\\begin{array}{l}\n\t\t\\langle\\mathcal{A}[u], v\\rangle \\triangleq \\int_{\\Omega}\\left(\\nabla u \\cdot \\nabla v+u \\cdot  v-f \\cdot v\\right) \\mathrm{d} x=0 \\\\\n\t\t\\mathcal{B}[u]=0, \\quad \\text { on } \\partial \\Omega\n\t\\end{array}\\right.\n\t\\end{equation*}\n算子 $\\mathcal{A}$ 的模可定义为 $\\|\\mathcal{A}[u]\\|_{o p} \\triangleq \\max \\left\\{\\langle\\mathcal{A}[u], v\\rangle /\\|v\\|_2 \\mid v \\in H_0^1, v \\neq 0\\right\\}$，\n则原方程项等价于如下优化问题形式\n$$\n\t\\min _{u \\in H^1}\\|\\mathcal{A}[u]\\|_{\\text {op }}^2 \\Longleftrightarrow \\min _{u \\in H^1} \\max _{v \\in H_0^1}|\\langle\\mathcal{A}[u], v\\rangle|^2 /\\|v\\|_2^2\n$$\n对右端取$log$，得到方程项的损失函数\n$$L_{\\text {int }}(\\theta, \\eta) \\triangleq \\log \\left|\\left\\langle\\mathcal{A}\\left[u_\\theta\\right], v_\\eta\\right\\rangle\\right|^2-\\log \\left\\|v_\\eta\\right\\|_2^2$$\n再类似PINN方法，构造边界条件罚项\n$$\n\tL_{\\text {bdry }}(\\theta) \\triangleq\\left(1 / N_b\\right) \\cdot \\sum_{j=1}^{N_b}\\left|u_\\theta\\left(x_b^{(j)}\\right)-g\\left(x_b^{(j)}\\right)\\right|^2 \\\\\n$$\n最终的优化问题就可写为\n$$\n\\min _\\theta \\max _\\eta L(\\theta, \\eta), \\quad \\text { where } L(\\theta, \\eta) \\triangleq L_{\\text {int }}(\\theta, \\eta)+\\alpha L_{\\text {bdry }}(\\theta)\n$$\n- 变分形式: Deep Ritz Method[<sup>4</sup>](#R4) (DRM) 方法的损失函数则是将势能函数加上边界条件罚项得到的，数值计算、优化求解和上述方法都是一致的，在此不赘述。","metadata":{},"id":"c0577dc6-9789-4845-8e68-53f5f295e6af"},{"cell_type":"markdown","source":"### 最经典的神经网络方法——PINN[<sup>2</sup>](#R2)\n\n作为神经网络方法的典型例子，我们将介绍该领域中最广为人知的PINN方法。\n\n为与下文的RFM进行对比，PINN的网络结构我们使用有单隐层的全连接神经网络，网络模型与权重初始化方式定义如下","metadata":{},"id":"47b29536-db95-44ce-8eb5-2ba43a06b2f4"},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\ntorch.set_default_dtype(torch.float64)\n\nR_m = 1.0\ndef weights_init(m):\n    if isinstance(m, (nn.Conv2d, nn.Linear)):\n        nn.init.uniform_(m.weight, a = -R_m, b = R_m)\n        nn.init.uniform_(m.bias, a = -R_m, b = R_m)\n\n# network definition\nclass Network(nn.Module):\n    def __init__(self, d, M):\n        super(Network, self).__init__()\n        self.fc_layer = nn.Sequential(nn.Linear(d, M, bias=True),nn.Tanh())\n        self.output_layer = nn.Linear(M, 1, bias = False)\n        \n    def forward(self, x):\n        h = self.fc_layer(x)\n        out = self.output_layer(h)\n        return out","metadata":{},"execution_count":4,"outputs":[],"id":"1af76ce6-88a1-4e89-a36f-0436c6120717"},{"cell_type":"markdown","source":"有了模型，下一步自然是构造损失函数。正如上文中所提到的，PINN的损失函数由如下\n$$\nL(u)=\\left\\|(\\Delta + \\lambda^2) u - f\\right\\|_{2,\\Omega}^2+\\lambda\\|u-U\\|_{2, \\partial \\Omega}^2\n$$\n在实际实现中，我们同样要对方程进行离散，在配点上配置损失函数（优化问题），展开写为如下形式：\n$$\nL(u)=\\sum_{i=1}^{N-1}((\\Delta + \\lambda^2) u(x_i) - f(x_i))^2+\\lambda_l(u(x_0)-U(x_0))^2+\\lambda_r(u(x_N)-U(x_N))^2\n$$","metadata":{},"id":"913728bc-d099-468c-9646-3f56d43c07a6"},{"cell_type":"code","source":"# loss definition\nclass Loss():\n    def __init__(self, net, left_boundary, right_boundary):\n        self.net = net\n        self.left_boundary = left_boundary\n        self.right_boundary = right_boundary\n\n    def sample(self, N):\n        x = torch.tensor(np.float64(np.linspace(self.left_boundary, self.right_boundary, N+1)[1:N]),requires_grad=True).reshape(-1,1)\n        x_boundary_left = torch.ones([1],requires_grad=True) * self.left_boundary\n        x_boundary_right = torch.ones([1],requires_grad=True) * self.right_boundary\n        return x, x_boundary_left, x_boundary_right\n\n    def loss_func(self, N):\n        # helmholtz equation\n        x, x_boundary_left, x_boundary_right = self.sample(N)\n        x = Variable(x, requires_grad=True)\n        y = self.net(x)\n        dx = torch.autograd.grad(y, x, grad_outputs=torch.ones_like(self.net(x)), create_graph=True)[0].reshape(-1, 1)\n        dxx = torch.autograd.grad(dx, x, grad_outputs=torch.ones_like(dx), create_graph=True)[0].reshape(-1, 1)\n        \n        f = -AA*(aa*aa+bb*bb)*torch.sin(bb*(x+0.05))*torch.cos(aa*(x+0.05))\\\n           -2.0*AA*aa*bb*torch.cos(bb*(x+0.05))*torch.sin(aa*(x+0.05))\\\n           -lamb * (AA * torch.sin(bb * (x + 0.05)) * torch.cos(aa * (x + 0.05)) + 2.0)\n        diff_error = (dxx - lamb*self.net(x) - f.reshape(-1, 1))**2\n        \n        # boundary condition\n        bd_left_error = (self.net(x_boundary_left) - (AA * torch.sin(bb * (x_boundary_left + 0.05)) * torch.cos(aa * (x_boundary_left + 0.05)) + 2.0)) ** 2\n        bd_right_error = (self.net(x_boundary_right) - (AA * torch.sin(bb * (x_boundary_right + 0.05)) * torch.cos(aa * (x_boundary_right + 0.05)) + 2.0)) ** 2\n        \n        return torch.mean(diff_error + bd_left_error + bd_right_error)","metadata":{},"execution_count":5,"outputs":[],"id":"e49641b1-e67e-44d0-af96-b9e91551151d"},{"cell_type":"markdown","source":"最后的优化部分，由于是非凸优化问题，我们直接使用通用的Adam优化算法求解，训练过程可以定义如下","metadata":{},"id":"767bee68-e3a5-4ad7-9ad7-c44b0e6fbb2a"},{"cell_type":"code","source":"# training process\nclass Train():\n    def __init__(self, net, loss, N):\n        self.errors = []\n        self.N = N\n        self.net = net\n        self.model = loss\n\n    def train(self, epoch, lr):\n        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.net.parameters()), lr)\n        avg_loss = 0\n        for e in range(epoch):\n            optimizer.zero_grad()\n            loss = self.model.loss_func(self.N)\n            avg_loss = avg_loss + float(loss.item())\n            loss.backward()\n            optimizer.step()\n            if e % 50 == 49:\n                loss = avg_loss/50\n                #print(\"Epoch {} - lr {} -  loss: {}\".format(e, lr, loss))\n                avg_loss = 0\n                error = self.model.loss_func(self.N)\n                self.errors.append(error.detach())","metadata":{},"execution_count":6,"outputs":[],"id":"65656fa7-721a-42bf-abc6-b24752f95368"},{"cell_type":"markdown","source":"最后，我们定义了测试过程，并同样在线性系统规模分别为 $100,200,400,800,1600,3200$ 时进行测试，以便与有限差分方法做对比","metadata":{},"id":"c89f624f-26b9-439b-966d-fa4c18f8a725"},{"cell_type":"code","source":"# test process\ndef Test(net, N):\n    # PINN testing process\n    points = np.linspace(left_boundary, right_boundary, N+1, dtype=np.float64)[:N]\n    true_value = u(points).reshape([-1,1])\n    numerical_value = net(torch.tensor(points,requires_grad=False).reshape([-1,1])).detach().cpu().numpy()\n    epsilon = true_value - numerical_value\n    epsilon = np.abs(epsilon)\n    error_inf = epsilon.max()\n    error_l2 = math.sqrt(8*sum(epsilon*epsilon)/len(epsilon))\n    print('L_infty=',error_inf,'L_2=',error_l2)\n    return(error_l2)\n\nif __name__==\"__main__\":\n    PINN_Error = np.zeros([5,3])\n    for i in range(5):\n        time_begin = time.time()\n        N = int(100 * 2**i) # number of collocation points\n        M = int(100 * 2**i) # number of basis\n        \n        # PINN model define and initialization\n        net = Network(1, M)\n        net.fc_layer = net.fc_layer.apply(weights_init)\n        \n        # PINN training process\n        FREEZE = False\n        if FREEZE == True:\n            net.fc_layer.requires_grad_(False)\n            loss = Loss(net, left_boundary, right_boundary)\n            train = Train(net, loss, N)\n            train.train(epoch=10**2, lr=0.001)\n            train.train(epoch=4*10**2, lr=0.0001)\n        else:\n            loss = Loss(net, left_boundary, right_boundary)\n            train = Train(net, loss, N)\n            train.train(epoch=5*10**3, lr=0.1)\n            train.train(epoch=10**4, lr=0.01)\n            train.train(epoch=10**4, lr=0.001)\n            train.train(epoch=2*10**4, lr=0.0001)\n        \n        # PINN testing process\n        print('***********************')\n        print(\"N = M =\",N)\n        PINN_Error[i,0] = N\n        PINN_Error[i,1] = Test(net, N)\n        time_end = time.time()\n        PINN_Error[i,2] = time_end - time_begin\n\n    error_plot([FDM_Error, PINN_Error])\n    time_plot([FDM_Error, PINN_Error])","metadata":{},"execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":"***********************\nN = M = 100\nL_infty= 0.10487853885758636 L_2= 0.0594229909371562\n***********************\nN = M = 200\nL_infty= 0.01510564043982976 L_2= 0.00966925733986969\n***********************\nN = M = 400\nL_infty= 0.03488366954326194 L_2= 0.023427789973332842\n***********************\nN = M = 800\nL_infty= 0.007590964691803981 L_2= 0.006726670710635236\n***********************\nN = M = 1600\nL_infty= 0.01069138024637617 L_2= 0.00775340961692466\n"},{"output_type":"display_data","data":{"remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17477/3b0d30277b2b4019ba6f789c817991d5/f58bdb047bf444babb4d940dac363c6f.png"},"metadata":{}},{"output_type":"display_data","data":{"remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17477/cfb2542c9f2546d1a0b216ee3ba8a8d3/4cb66baa302c4e4b9deb2428e5986de2.png"},"metadata":{}}],"id":"b0313499-11b5-4b31-af1b-cc682d62f1fa"},{"cell_type":"markdown","source":"### 小结（机器学习方法）\n1. 从误差对比图可以看出，通过简单网络架构实现的<font color=\"Blue\">PINN在任何自由度下结果都可接受，但误差并不随模型自由度的增加而下降</font>，这可能是由于模型与训练参数并没有精细调整，但更主要的原因是PINN的训练过程是非凸非线性的优化过程，<font color=\"Blue\">容易陷入局部最优</font>；且<font color=\"Blue\">罚参数的调整较为依赖经验</font>，并不直接；\n1. 从时间对比图可以看出，不仅误差不随自由度的增大而下降，<font color=\"Blue\">PINN的训练时间也远远超过经典有限差分方法的求解时间</font>，这也是本节所介绍的机器学习方法的通病；\n1. 尽管在一维Helmholtz问题上没有体现，但由于机器学习方法无需事先生成网格，只需在计算域中采点进行训练，因此<font color=\"Red\">不管在复杂几何的问题还是高维问题中，机器学习方法都更占优势</font>；\n1. 由于机器学习方法通常是以可微分的映射表示，因此<font color=\"Red\">可以在同一框架下统一地处理正反问题</font>，也<font color=\"Red\">可以用作算子的参数化</font>；\n1. 回归到PINN框架下，一些文献[<sup>6</sup>](#R6)说明其原始模型中所构造的损失函数并不适用于所有方程，构造其他损失函数和训练算法后结果明显变好；\n1. 总的来说，基于机器学习的微分方程数值方法还存在较多的改进空间。\n","metadata":{},"id":"78e82c54-5746-4570-ba1d-4696928fc8b0"},{"cell_type":"markdown","source":"## 随机特征方法：The random feature method[<sup>5</sup>](#R5) <a id='layer2'></a>\n\n从上文对经典方法和机器学习方法的介绍，我们可以看出：经典方法有稳定的收敛阶，但难以处理复杂几何或高维问题；机器学习方法能处理复杂几何和高维问题，但求解时间长、求解精度有限。因此，如何结合经典方法与机器学习方法的优势，提出同时兼具精度和处理复杂几何能力的方法在科学计算领域一直是重要的问题。\n\n作为一种新型的微分方程数值方法，本文的主角：随机特征方法（The random feature method, RFM）融合了经典方法和机器学习方法的优势，能够实现谱精度（semi-log误差图中线性），其无网格的特性也让它能够容易地处理复杂几何上的问题，是一种兼具收敛性和简便性的新型微分方程数值方法。\n     \n类似前文两类方法的主要构成，RFM的主要构成同样分成如下三部分\n- 数值离散：基于方程强形式，在配点上构造线性系统\n- 逼近空间：局部随机特征函数，并用单位分解技术将其组合\n- 优化问题：线性最小二乘问题，容易求解，罚参数易于调整\n\n下文我们将以对应的三小节逐步构建RFM的框架。","metadata":{},"id":"b4384754-c501-46cc-9826-335ea1fd4c3b"},{"cell_type":"markdown","source":"### 逼近空间\n\n#### **随机特征函数**\n什么是随机特征函数（random feature function, RFF）？随机特征函数就是特征向量随机生成的函数。对于机器学习方法来说，就是网络权值随机初始化的步骤。因此从机器学习的框架下看，RFM就是利用 $M$ 个定义在 $\\Omega$ 上的网络基函数 $\\{\\phi_m\\}$ 的线性组合来表示数值解\n\t\\begin{equation*}\\label{eqn:rfm}\n\t\tu_M(\\boldsymbol{x}) = \\sum_{m=1}^M u_m \\phi_m(\\boldsymbol{x})\n\t\\end{equation*}\n\t$$\n\t\\phi_m(\\boldsymbol{x}) = \\sigma(\\boldsymbol{k}_m \\cdot\\boldsymbol{x} + b_m)\n\t$$\n其中 $\\boldsymbol{k}_m, b_m$ 就是随机生成后固定的内层参数，而 $\\sigma$ 是非线性激活函数，为方程求解提供非线性的部分。\n\n#### **局部RFF与单位分解**\n上述随机特征函数是全局定义的，但微分方程的解常常有小尺度的局部变化，因此RFM考虑在多个局部构造局部随机特征函数，再用单位分解（partition of unity）技术将它们组合。\n\n具体来说，首先取定单位分解函数的中心点 $\\{\\boldsymbol{x}_n\\}_{n=1}^{M_p}\\subset\\Omega$，在这 $M_p$ 个局部构造仿射变换\n\\begin{equation}\n\t\\tilde{\\boldsymbol{x}}=\\frac{1}{\\boldsymbol{r}_{n}}(\\boldsymbol{x}-\\boldsymbol{x}_{n}), \\quad n=1,\\cdots, M_p,\n\t\\end{equation}\n这个仿射变换将 $[x_{n1}-r_{n1},x_{n1}+r_{n1}]\\times \\cdots \\times [x_{nd}-r_{nd},x_{nd}+r_{nd}]$ 的小局部映射到统一的区间 $[-1,1]^{d}$，以便实现局部特征的拟合。而单位分解函数的构造依赖于这个仿射变换，常取为\n\\begin{equation*}\n\t\\psi_{n}^{a}(x)=\\mathbb{I}_{-1 \\leq \\tilde{x} < 1},\n\t\\label{psi1}\n\t\\end{equation*}\n\t或\n\t\\begin{equation*}\n\t\\psi_{n}^{b}(x) =\\mathbb{I}_{\\left[-\\frac{5}{4},-\\frac{3}{4}\\right]}(\\tilde{x}) \\frac{1+\\sin (2 \\pi \\tilde{x})}{2}+\\mathbb{I}_{\\left[-\\frac{3}{4}, \\frac{3}{4}\\right]}(\\tilde{x})+\\mathbb{I}_{\\left[\\frac{3}{4}, \\frac{5}{4}\\right]}(\\tilde{x}) \\frac{1-\\sin (2 \\pi \\tilde{x})}{2}\n\t\\end{equation*}\n    \n<img src=\"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17477/bea71b3b2ef34af9828b21e52ae60d12/63fc00d5-b4d1-4089-8faa-3659a46a4dce.png\"  width=\"300\" alt=\"两类一维PoU函数示例\">\n紧接着，在每个局部定义 $J_n$ 个随机特征函数\n\t\\begin{equation}\\label{eqn:basis0}\n\t\\phi_{nj}(\\boldsymbol{x}) = \\sigma(\\boldsymbol{k}_{nj} \\cdot \\tilde{\\boldsymbol{x}} + b_{nj}), \\quad j=1, \\cdots, J_n,\n\t\\end{equation}\n\n则最终数值解就是将局部随机特征函数通过单位分解函数组合起来\n\\begin{equation*}\n\t\tu_M(\\boldsymbol{x})=\\sum_{n=1}^{M_p} \\psi_n (\\boldsymbol{x})   \\sum_{j=1}^{J_n }u_{nj} \\phi_{nj} (\\boldsymbol{x})\n\t\t\\label{representation2}\n\t\t\\end{equation*}\n        \n*以下实验代码使用$\\psi_{n}^{a}(x)$作为单位分解函数进行演示*","metadata":{},"id":"ed715f9f-0b4a-4fb1-8e3e-32ff1c6a3aaa"},{"cell_type":"code","source":"# computational domain\nX_min = 0.0\nX_max = 8.0\n\n# random initialization for parameters\ndef weights_init(m):\n    if isinstance(m, (nn.Conv2d, nn.Linear)):\n        nn.init.uniform_(m.weight, a = -R_m, b = R_m)\n        nn.init.uniform_(m.bias, a = -R_m, b = R_m)\n\n# random feature basis when using \\psi^{a} as PoU function\nclass RFM_rep_a(nn.Module):\n    def __init__(self, d, J_n, x_max, x_min):\n        super(RFM_rep_a, self).__init__()\n        self.d = d\n        self.J_n = J_n\n        self.r = (x_max - x_min) / 2.0\n        self.x_c = (x_max + x_min)/2\n        self.phi = nn.Sequential(nn.Linear(self.d, self.J_n, bias=True),nn.Tanh())\n\n    def forward(self,x):\n        x = (x - self.x_c) / self.r\n        x = self.phi(x)\n        return x\n\n# random feature basis when using \\psi^{b} as PoU function\nclass RFM_rep_b(nn.Module):\n    def __init__(self, d, J_n, x_max, x_min):\n        super(RFM_rep_b, self).__init__()\n        self.d = d\n        self.J_n = J_n\n        self.n = x_min/(X_max-X_min) * M_p\n        self.r = (x_max - x_min) / 2.0\n        self.x_0 = (x_max + x_min)/2\n        self.phi = nn.Sequential(nn.Linear(self.d, self.J_n, bias=True),nn.Tanh())\n\n    def forward(self,x):\n        d = (x - self.x_0) / self.r\n        psi = ((d <= -3/4) & (d > -5/4)) * (1+torch.sin(2*np.pi*d))/2 + ((d <= 3/4) & (d > -3/4)) * 1.0 + ((d <= 5/4) & (d > 3/4)) * (1-torch.sin(2*np.pi*d))/2\n        y = self.phi(d)\n        if self.n == 0:\n            psi = ((d <= 3/4) & (d > -1)) * 1.0 + ((d <= 5/4) & (d > 3/4)) * (1-torch.sin(2*np.pi*d))/2\n        elif self.n == M_p-1:\n            psi = ((d <= -3/4) & (d > -5/4)) * (1+torch.sin(2*np.pi*d))/2 + ((d <= 1) & (d > -3/4)) * 1.0\n        else:\n            psi = ((d <= -3/4) & (d > -5/4)) * (1+torch.sin(2*np.pi*d))/2 + ((d <= 3/4) & (d > -3/4)) * 1.0 + ((d <= 5/4) & (d > 3/4)) * (1-torch.sin(2*np.pi*d))/2\n        return(psi*y)\n\n# predefine the random feature functions in each PoU region\ndef pre_define(M_p,J_n):\n    models = []\n    for n in range(M_p):\n        x_min = (X_max-X_min)/M_p * n + X_min\n        x_max = (X_max-X_min)/M_p * (n+1) + X_min\n        model = RFM_rep_a(d = 1, J_n = J_n, x_min = x_min, x_max = x_max)\n        model = model.apply(weights_init)\n        model = model.double()\n        for param in model.parameters():\n            param.requires_grad = False\n        models.append(model)\n    return(models)","metadata":{},"execution_count":8,"outputs":[],"id":"e3843cb7-51b5-459c-a3f8-b378f69c1f93"},{"cell_type":"markdown","source":"### 数值离散：线性系统构造\n\n在RFM中，损失函数同样在配点上进行计算，和PINN的损失函数构造完全一致，以最小二乘形式计算方程项，并加上边界的罚项\n\\begin{equation}\n\tLoss = \\sum_{\\boldsymbol{x}_{i} \\in C_{I}}\\sum_{k=1}^{K_I}\\lambda_{Ii}^{k}\\|\\mathcal{L}^{k}\\boldsymbol{u}(\\boldsymbol{x}_{i})-\\boldsymbol{f}^{k}(\\boldsymbol{x}_{i})\\|_{l^{2}}^{2}+ \\sum_{\\boldsymbol{x}_{j} \\in C_{B}}\\sum_{\\ell=1}^{K_B}\\lambda_{Bj}^{\\ell}\\|\\mathcal{B}^{\\ell}\\boldsymbol{u}(\\boldsymbol{x}_{j})-\\boldsymbol{g}^{\\ell}(\\boldsymbol{x}_{j})\\|_{l^{2}}^{2}.\n\t\\label{loss2}\n\t\\end{equation}\n\n虽然损失函数形式上看完全类似，但RFM与PINN构造的优化问题则完全不同，RFM求解的是内层参数固定、仅优化最外层线性参数的线性优化问题。从经典数值算法的角度来看，RFM构造的是线性最小二乘问题，该问题可以表为线性系统 $Au=f$ 的形式。因此罚参数 $\\lambda_i$ 的调整可以直接基于矩阵 $A$ 的信息，对于每个配点、每个方程，<font color=\"Red\">RFM允许他们有不同的罚参数</font>。\n\n对应于一维Helmholtz方程，RFM的损失函数可以写为\n$$\nL(u)=\\sum_{i=1}^{N-1}\\lambda_i((\\Delta + \\lambda^2) u(x_i) - f(x_i))^2+\\lambda_0(u(x_0)-U(x_0))^2+\\lambda_N(u(x_N)-U(x_N))^2\n$$\n\n在实际代码实现中，我们通过下述配点上的等式构造线性系统 $Au=f$ 来等价地求解该优化问题\n\\begin{equation*}\n\t\t\\left\\{\\begin {aligned}\n\t\t(\\Delta + \\lambda^2) u(x_i) &= f(x_i), \\quad &&n=1,\\cdots,N-1\\\\\n\t\tu(x_{i})  &= U(x_{i}) \\quad && n=0,N\n\t\t\\end {aligned}\\right.\n\t\t%\\label{stokesflow}\n\t\\end{equation*}\n\n*需要注意的是，在单位分解函数的连续性不满足方程解所需的连续性时，需要额外加入一组连续性条件。例如：在使用$\\psi^{a}$求解二阶方程时，需要在不同单位分解计算区域的边缘加入零阶和一阶连续性条件*","metadata":{},"id":"4e00d11d-ce81-4621-91b1-7ab89fc4d9f0"},{"cell_type":"code","source":"# Assembling the matrix A,f in linear system 'Au=f'\ndef assemble_matrix(models,points,M_p,J_n,Q,lamb):\n    A_I = np.zeros([M_p*Q, M_p*J_n]) # PDE term\n    A_B = np.zeros([2, M_p*J_n]) # boundary condition\n    A_C_0 = np.zeros([M_p-1, M_p*J_n]) # 0-order smoothness condition\n    A_C_1 = np.zeros([M_p-1, M_p*J_n]) # 1-order smoothness condition\n    f = np.zeros([M_p*Q + 2*(M_p - 1) + 2, 1])\n    \n    for n in range(M_p):\n        # forward and grad\n        point = torch.tensor(points[n], requires_grad=True)\n        out = models[n](point)\n        values = out.detach().numpy()\n        value_l, value_r = values[0,:], values[-1,:]\n        grad1 = []\n        grad2 = []\n        for i in range(J_n):\n            g1 = torch.autograd.grad(outputs=out[:,i], inputs=point,\n                                  grad_outputs=torch.ones_like(out[:,i]),\n                                  create_graph = True, retain_graph = True)[0]\n            grad1.append(g1.squeeze().detach().numpy())\n            \n            g2 = torch.autograd.grad(outputs=g1[:,0], inputs=point,\n                                  grad_outputs=torch.ones_like(out[:,i]),\n                                  create_graph = False, retain_graph = True)[0]\n            grad2.append(g2.squeeze().detach().numpy())\n        grad1 = np.array(grad1).T\n        grad2 = np.array(grad2).T\n        grad_l = grad1[0,:]\n        grad_r = grad1[-1,:]\n        Lu = grad2 - lamb * values\n        \n        # Lu = f condition\n        A_I[n*Q:(n + 1)*Q, n*J_n:(n + 1)*J_n] = Lu[:Q,:]\n        f[n*Q:(n + 1)*Q, :] = F(points[n], lamb).reshape([-1,1])[:Q]\n        \n        # boundary conditions\n        if n == 0:\n            A_B[0, :J_n] = value_l\n        if n == M_p-1:\n            A_B[1, -J_n:] = value_r\n        \n        # smoothness conditions\n        if M_p > 1:\n            if n == 0 :\n                A_C_0[0, :J_n] = -value_r\n                A_C_1[0, :J_n] = -grad_r\n            elif n == M_p - 1:\n                A_C_0[M_p - 2, -J_n:] = value_l\n                A_C_1[M_p - 2, -J_n:] = grad_l\n            else:\n                A_C_0[n-1,n*J_n:(n + 1)*J_n] = value_l\n                A_C_1[n-1,n*J_n:(n + 1)*J_n] = grad_l\n                A_C_0[n,n*J_n:(n + 1)*J_n] = -value_r\n                A_C_1[n,n*J_n:(n + 1)*J_n] = -grad_r\n    if M_p > 1:\n        A = np.concatenate((A_I,A_B,A_C_0,A_C_1),axis=0)\n    else:\n        A = np.concatenate((A_I,A_B),axis=0)\n    \n    # boundary conditions\n    f[M_p*Q,:] = u(0.)\n    f[M_p*Q+1,:] = u(8.)\n    \n    return(A,f)","metadata":{},"execution_count":9,"outputs":[],"id":"4add1f4a-4648-45a4-a97e-8b6c6ce2212d"},{"cell_type":"markdown","source":"### 优化问题\n\n由于一些实际问题中物理参数变化较大，因此RFM基于矩阵 $A$ 的信息（将矩阵 $A$ 每行最大值rescaling到同一规模），使用如下自动调参方案来平衡损失函数中各项的贡献，其中罚参数设为\n\t\\begin{align*}\n\t\t\t&\\lambda_{Ii}^{k} = \\frac{c}{\\underset{1\\leq n\\leq M_p}{\\max}\\underset{1\\leq j'\\leq J_n}{\\max}\\underset{1\\leq k'\\leq K_I}{\\max}|\\mathcal{L}^{k} (\\phi^{k'}_{nj'}(\\boldsymbol{x}_{i})\\psi_{n}(\\boldsymbol{x}_{i}))|}\\quad \\boldsymbol{x}_{i} \\in C_{I}, \\; k = 1,\\cdots, K_I\\\\\n\t\t\t&\\lambda_{Bj}^{\\ell} = \\frac{c}{\\underset{1\\leq n\\leq M_p}{\\max}\\underset{1\\leq j'\\leq J_n}{\\max}\\underset{1\\leq \\ell'\\leq K_I}{\\max}|\\mathcal{B}^{\\ell} (\\phi^{\\ell'}_{nj'}(\\boldsymbol{x}_{j})\\psi_{n}(\\boldsymbol{x}_{j}))|}\\quad \\boldsymbol{x}_{j} \\in C_{B}, \\; \\ell = 1,\\cdots, K_B \n\t\t\\end{align*}\n\n对应于一维Helmholtz方程，这些罚参数可以写为\n\t\\begin{align*}\n\t\t\t&\\lambda_{i} = \\frac{100}{\\underset{1\\leq n\\leq M_p}{\\max}\\underset{1\\leq j'\\leq J_n}{\\max}|(\\Delta + \\lambda^2) (\\phi_{nj'}(x_{i})\\psi_{n}(x_{i}))|}\\quad i = 1,\\cdots, N-1\\\\\n\t\t\t&\\lambda_{i} = \\frac{100}{\\underset{1\\leq n\\leq M_p}{\\max}\\underset{1\\leq j'\\leq J_n}{\\max}|\\phi_{nj'}(x_{i})\\psi_{n}(x_{i})|}\\quad i = 0,N\n\t\t\\end{align*}","metadata":{},"id":"693d85ca-e911-4f50-998c-20b5b66f45c9"},{"cell_type":"code","source":"from scipy.linalg import lstsq\ndef main(M_p, J_n, Q, lamb):\n    # prepare collocation points\n    time_begin = time.time()\n    points = []\n    for n in range(M_p):\n        x_min = (X_max-X_min)/M_p * n + X_min\n        x_max = (X_max-X_min)/M_p * (n+1) + X_min\n        points.append(np.linspace(x_min, x_max, Q+1).reshape([-1,1]))\n    \n    # prepare models\n    models = pre_define(M_p,J_n)\n    \n    # matrix define (Au=f)\n    A,f = assemble_matrix(models, points, M_p, J_n, Q, lamb)\n    print('***********************')\n    print('Matrix shape: N=%s,M=%s'%(A.shape))\n    # rescaling\n    c = 100.0\n    for i in range(len(A)):\n        ratio = c/A[i,:].max()\n        A[i,:] = A[i,:]*ratio\n        f[i] = f[i]*ratio\n    \n    # solve\n    w = lstsq(A,f)[0]\n    \n    # test\n    error = test(models,M_p,J_n,Q,w)\n    \n    time_end = time.time()\n    return(error, time_end - time_begin)","metadata":{},"execution_count":10,"outputs":[],"id":"4e8ed457-3f6a-47b4-9e19-cf528bf39fdb"},{"cell_type":"markdown","source":"### 实验1：与有限差分方法 / PINN方法的对比实验\n经过上面的准备，我们已搭建完RFM的整体框架。同样地，我们定义RFM的测试过程，实现 $[0,8]$ 区间上的一维Helmholtz方程求解","metadata":{},"id":"211a4eee-d103-4cb4-a2df-b2cddb958db0"},{"cell_type":"code","source":"# analytical solution parameters\nAA = 1\naa = 2.0*np.pi\nbb = 3.0*np.pi\nlamb = 4\n\nvanal_u = np.vectorize(u)\nvanal_d2u_dx2 = np.vectorize(d2u_dx2)\n\ndef F(points, lamb):\n    return(vanal_d2u_dx2(points) - lamb * vanal_u(points))\n\nimport math\nimport matplotlib.pyplot as plt\n# calculate the l^{infty}-norm and l^{2}-norm error for u\ndef test(models,M_p,J_n,Q,w,plot = False):\n    epsilon = []\n    true_values = []\n    numerical_values = []\n    test_Q = 2*Q\n    for n in range(M_p):\n        points = torch.tensor(np.linspace((X_max-X_min)/M_p * n + X_min, (X_max-X_min)/M_p * (n+1) + X_min, test_Q+1),requires_grad=False).reshape([-1,1])\n        out = models[n](points)\n        values = out.detach().numpy()\n        true_value = vanal_u(points.numpy()).reshape([-1,1])\n        numerical_value = np.dot(values, w[n*J_n:(n+1)*J_n,:])\n        true_values.extend(true_value)\n        numerical_values.extend(numerical_value)\n        epsilon.extend(true_value - numerical_value)\n    true_values = np.array(true_values)\n    numerical_values = np.array(numerical_values)\n    epsilon = np.array(epsilon)\n    epsilon = np.maximum(epsilon, -epsilon)\n    print('R_m=%s,M_p=%s,J_n=%s,Q=%s'%(R_m,M_p,J_n,Q))\n    print('L_infty error =',epsilon.max(),', L_2 error =',math.sqrt(8*sum(epsilon*epsilon)/len(epsilon)))\n    x = [((X_max - X_min)/M_p)*i / test_Q  for i in range(M_p*(test_Q+1))]\n    return(math.sqrt((X_max-X_min)*sum(epsilon*epsilon)/len(epsilon)))\n\nif __name__ == '__main__':\n    lamb = 4\n    R_m = 3\n    J_n = 50 # the number of basis functions per PoU region\n    Q = 50 # the number of collocation pointss per PoU region\n    RFM_Error = np.zeros([5,3])\n    for i in range(5): # the number of PoU regions\n        M_p = 2*(2**i)\n        RFM_Error[i,0] = int(M_p * J_n)\n        RFM_Error[i,1], RFM_Error[i,2] = main(M_p,J_n,Q,lamb)\n    error_plot([FDM_Error, PINN_Error, RFM_Error])\n    time_plot([FDM_Error, PINN_Error, RFM_Error])","metadata":{},"execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":"***********************\nMatrix shape: N=104,M=100\nR_m=3,M_p=2,J_n=50,Q=50\nL_infty error = 39.73441344537901 , L_2 error = 30.231211492127343\n***********************\nMatrix shape: N=208,M=200\nR_m=3,M_p=4,J_n=50,Q=50\nL_infty error = 0.002640724565257102 , L_2 error = 0.0019328785522688856\n***********************\nMatrix shape: N=416,M=400\nR_m=3,M_p=8,J_n=50,Q=50\nL_infty error = 5.530362181538351e-07 , L_2 error = 5.391978823035383e-07\n***********************\nMatrix shape: N=832,M=800\nR_m=3,M_p=16,J_n=50,Q=50\nL_infty error = 4.257653940520356e-08 , L_2 error = 3.06632546585445e-08\n***********************\nMatrix shape: N=1664,M=1600\nR_m=3,M_p=32,J_n=50,Q=50\nL_infty error = 1.0577603237749145e-09 , L_2 error = 1.241129484555056e-09\n"},{"output_type":"display_data","data":{"remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17477/a52a847a205d4b51812155fd54bdac66/58ff4cabf52d49d290c25de2f3494950.png"},"metadata":{}},{"output_type":"display_data","data":{"remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17477/9e0859781f6b44f183b66a56d7a7af19/7c6d7171b0f84c41a1343a35de7a409b.png"},"metadata":{}}],"id":"e9a313e8-dfd6-4074-adad-97e93bc92447"},{"cell_type":"markdown","source":"**由上面的结果，我们可以发现：在相同的矩阵规模下，RFM的结果远好于有限差分方法与PINN方法的结果。**\n\n### 实验2：RFM谱精度实验\n为展现RFM的谱精度，我们取较小的矩阵规模，再次运行并绘制semi-log误差图如下\n\n*我们重新定义了部分函数，删除输出以便多次实验取均值*","metadata":{},"id":"f824f31c-d6ca-4bd5-99c8-34c18908b544"},{"cell_type":"code","source":"def test(models,M_p,J_n,Q,w,plot = False):\n    epsilon = []\n    true_values = []\n    numerical_values = []\n    test_Q = 2*Q\n    for n in range(M_p):\n        points = torch.tensor(np.linspace((X_max-X_min)/M_p * n + X_min, (X_max-X_min)/M_p * (n+1) + X_min, test_Q+1),requires_grad=False).reshape([-1,1])\n        out = models[n](points)\n        values = out.detach().numpy()\n        true_value = vanal_u(points.numpy()).reshape([-1,1])\n        numerical_value = np.dot(values, w[n*J_n:(n+1)*J_n,:])\n        true_values.extend(true_value)\n        numerical_values.extend(numerical_value)\n        epsilon.extend(true_value - numerical_value)\n    true_values = np.array(true_values)\n    numerical_values = np.array(numerical_values)\n    epsilon = np.array(epsilon)\n    epsilon = np.maximum(epsilon, -epsilon)\n    #print('R_m=%s,M_p=%s,J_n=%s,Q=%s'%(R_m,M_p,J_n,Q))\n    #print('L_infty error =',epsilon.max(),', L_2 error =',math.sqrt(8*sum(epsilon*epsilon)/len(epsilon)))\n    x = [((X_max - X_min)/M_p)*i / test_Q  for i in range(M_p*(test_Q+1))]\n    return(math.sqrt((X_max-X_min)*sum(epsilon*epsilon)/len(epsilon)))\n\ndef main(M_p, J_n, Q, lamb):\n    # prepare collocation points\n    points = []\n    for n in range(M_p):\n        x_min = (X_max-X_min)/M_p * n + X_min\n        x_max = (X_max-X_min)/M_p * (n+1) + X_min\n        points.append(np.linspace(x_min, x_max, Q+1).reshape([-1,1]))\n    \n    # prepare models\n    models = pre_define(M_p,J_n)\n    \n    # matrix define (Au=f)\n    A,f = assemble_matrix(models, points, M_p, J_n, Q, lamb)\n    #print('***********************')\n    #print('Matrix shape: N=%s,M=%s'%(A.shape))\n    # rescaling\n    c = 100.0\n    for i in range(len(A)):\n        ratio = c/A[i,:].max()\n        A[i,:] = A[i,:]*ratio\n        f[i] = f[i]*ratio\n    \n    # solve\n    w = lstsq(A,f)[0]\n    \n    # test\n    return(A.shape[0],test(models,M_p,J_n,Q,w))\n\ndef error_plot_semi_log(Error):\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=[7, 5])\n    plt.tick_params(labelsize=10)\n    font2 = {\n    'weight' : 'normal',\n    'size'   : 22,\n    }\n    plt.xlabel('Degrees of freedom',font2)\n    plt.ylabel('$L_2$ absolute errors',font2)\n    plt.yscale('log')\n    plt.plot(Error[:,0], Error[:,1], \\\n             color='black', lw=1.5, ls='-', clip_on=False,\\\n             marker='o', markersize=10, \\\n             markerfacecolor='none',\\\n             markeredgecolor='black',markeredgewidth=1.5)\n    plt.title(\"Spectral accuracy of RFM\")\n    plt.show()\n\nif __name__ == '__main__':\n    lamb = 4\n    R_m = 3\n    M_p = 4 # the number of PoU regions\n    J_n = 50 # the number of basis functions per PoU region\n    \n    Error = np.zeros([8,2])\n    for i in range(8):\n        Q = int(10 * (i+1)) # the number of collocation pointss per PoU region\n        m = []\n        for o in range(40): # 40 experiments for mean\n            N, error = main(M_p,J_n,Q,lamb)\n            m.append(error)\n        Error[i,0] = N\n        Error[i,1] = np.array(m).mean()\n    error_plot_semi_log(Error)\n    ","metadata":{},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/17477/0289c5f3842844d7bd73206613b86b51/30674f9814d34ab498321dc0ec42c9b6.png"},"metadata":{}}],"id":"a93dbda2-ac54-478c-be29-b60d1e45675c"},{"cell_type":"markdown","source":"**由上面的结果，我们可以发现：在固定基函数数量的情况下，由小到大逐步增加配点个数，RFM的精度有明显的谱精度（semi-log误差图近似为直线）。**\n\n### 小结（RFM）\n1. 由实验1的误差对比图，我们可以看出在自由度一致的情况下，<font color=\"Red\">RFM的结果显著优于有限差分方法与PINN方法，精度更优且下降更快</font>；\n1. 实验2的semi-log误差图中，线性下降的误差曲线说明<font color=\"Red\">RFM实际上具有谱精度</font>；\n1. 由实验1的时间对比图，我们可以看出由于在线性最小二乘框架下进行计算，<font color=\"Red\">RFM的求解（优化）时间也远远少于PINN方法</font>，与有限差分方法约在同一量级；\n1. 与机器学习方法类似，RFM无需事先生成网格，只需在计算域中采点进行训练，因此<font color=\"Red\">RFM也能容易地处理复杂几何和高维问题</font>；\n1. RFM所构造的逼近空间同样是可微分映射，因此<font color=\"Red\">可以在同一框架下统一地处理正反问题</font>；\n1. 相比于PINN的训练过程，RFM的优化问题是线性最小二乘问题，所以<font color=\"Red\">罚参数的调整非常方便</font>；\n1. 在实验2中，我们可以发现在线性最小二乘的求解框架中，线性系统的行列数无需相等，这使得在构建线性系统时有更好的灵活性。","metadata":{},"id":"c8b04e07-09b6-4e16-858d-e11450506469"},{"cell_type":"markdown","source":"## 总结与讨论\n本文通过在一维Helmholtz方程上的实验对比，说明RFM是一种兼具稳定收敛性和简便性的新型微分方程数值方法。总的来说，RFM避免了经典数值算法中的网格生成步骤，更容易处理复杂几何和高维问题；通过引入随机特征函数，在线性最小二乘优化框架下求解优化问题，避免了机器学习方法中的非凸优化，使得其精度和效率都显著更好；可以说RFM为PDE数值求解提供了一个新的视角。\n除了文中的简单例子，RFM在高维的固体力学、流体力学、电磁场等实际应用中同样证明了其精度与效率[<sup>5</sup>](#R5)。\n\n在机器学习席卷了学术界和工业界的今天，神经网络模型的拟合能力使其革新了许多领域的范式，但对于高精度要求的微分方程求解来说，目前发展的机器学习方法还不够，甚至带来了一些新的问题。本文抛砖引玉，通过介绍RFM如何结合经典数值方法和机器学习方法的优势，设计出了一种新的计算范式，希望能给读者一些启发。在我们看来，如何将强大的机器学习方法，真正在微分方程求解问题、在科学计算领域用对、用好，真正发挥出它的长处，还是一个尚待探索的问题。","metadata":{},"id":"c8aae6e2-9b63-42fb-8ec0-da343585a049"},{"cell_type":"markdown","source":"## 参考文献\n<div><a name=\"R1\"></a>\n[1] J. A. Sirignano and K. Spiliopoulos, DGM: A deep learning algorithm for solving partial differential equations, Journal of Computational Physics, 375 (2018), pp. 1339–1364\n</div>\n<div><a name=\"R2\"></a>\n[2] M. Raissi, P. Perdikaris, and G. E. Karniadakis, Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations, Journal of Computational Physics, 378 (2019), pp. 686–707.\n</div>\n<div><a name=\"R3\"></a>\n[3] Y. Zang, G. Bao, X. Ye, and H. Zhou, Weak adversarial networks for highdimensional partial differential equations, Journal of Computational Physics, 411\n(2020), p. 109409.\n</div>\n<div><a name=\"R4\"></a>\n[4] W. E and B. Yu, The Deep Ritz Method: A Deep Learning-Based Numerical Algorithm for Solving Variational Problems, Communications in Mathematics and Statistics, 6 (2018), pp. 1–12.\n</div>\n<div><a name=\"R5\"></a>\n[5] J. Chen, X. Chi, W. E, and Z. Yang, Bridging traditional and machine learningbased algorithms for solving pdes: The random feature method, Journal of Machine\nLearning, 1 (2022), pp. 268–298.\n</div>\n<div><a name=\"R6\"></a>\n[6] C. Wang, S. Li, D. He, and L. Wang, Is $ L^ 2$ Physics Informed Loss Always Suitable for Training Physics Informed Neural Network?. Advances in Neural Information Processing Systems, 35 (2022), pp. 8278-8290.\n</div>","metadata":{},"id":"45fb84df-06da-4aa5-85d3-416fc2dbe74f"}]}